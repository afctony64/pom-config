# =============================================================================
# REMOTE MODE PROFILE
# =============================================================================
#
# Use when: Working remotely with Spark accessible via Cloudflare tunnel
#
# This profile configures the ecosystem to use:
#   - Spark GPU for Ollama via tunnel (localhost:11436 → tunnel → Spark)
#   - Mac native Transformers (Metal acceleration)
#   - Mac Weaviate for local collections
#   - Spark Weaviate via tunnel for collections
#
# To activate: Copy values to shared-config.env or source this file
#
# =============================================================================

# Mode Identifier
POMSPARK_MODE=remote

# =============================================================================
# WEAVIATE CONFIGURATION
# =============================================================================

# Primary Weaviate mode (local with cloud sync)
WEAVIATE_MODE=local

# Weaviate URLs
MAC_WEAVIATE_URL=http://mac-weaviate:8080
WEAVIATE_URL=http://localhost:8080

# Default target for new collections
DEFAULT_WEAVIATE_TARGET=cloud

# =============================================================================
# OLLAMA CONFIGURATION (LLM Inference)
# =============================================================================

# Primary Ollama URL (Spark via Cloudflare tunnel)
# Port 11436 is the tunnel endpoint on localhost
OLLAMA_URL=http://localhost:11436
SPARK_OLLAMA_URL=http://localhost:11436

# Mac native Ollama (fallback)
MAC_OLLAMA_URL=http://host.docker.internal:11435

# =============================================================================
# TRANSFORMERS CONFIGURATION (Embeddings)
# =============================================================================

# Primary Transformers URL (Mac native - Metal)
# Tunneling transformers has too much latency, use local
TRANSFORMERS_URL=http://host.docker.internal:8093
MAC_TRANSFORMERS_URL=http://host.docker.internal:8093

# =============================================================================
# LLM PROXY CONFIGURATION
# =============================================================================

LLM_PROXY_URL=http://pom-llm-proxy:8000

# =============================================================================
# OTHER SERVICES
# =============================================================================

REDIS_URL=redis://localhost:6379

# =============================================================================
# REMOTE MODE NOTES
# =============================================================================
#
# Requires:
#   - Cloudflare tunnel running on Spark server
#   - Tunnel configured for Ollama (port 11434)
#
# Start tunnel on Spark:
#   cloudflared tunnel run pom-ollama
#
# Performance expectations:
#   - Ollama: Good (Spark GPU via tunnel)
#   - Embeddings: Good (local Metal)
#   - Network-dependent latency on LLM calls
#
# =============================================================================
