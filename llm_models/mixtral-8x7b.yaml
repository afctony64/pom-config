id: "mixtral:8x7b"
name: Mixtral 8x7B Instruct
type: llm_model
version: "1.0.0"
provider:
  name: ollama
  api_type: chat
adapter_config:
  type: ollama
  model: mixtral:8x7b-instruct-v0.1-q4_K_M
  base_url: null
  base_url_env: null
parameters:
  temperature: 0.6
  max_tokens: 800
  top_p: 0.9
capabilities:
  structured_output: json_strict
  reasoning: false
  tool_calling: false
  streaming: true
  vision: false
  conversation_id: false
cost:
  per_1m_input_tokens: 0.0
  per_1m_output_tokens: 0.0
  currency: USD
  notes: Local Ollama inference
performance:
  avg_latency_ms: 4122.6
  tokens_per_second: 17
  context_window: 32768
metadata:
  description: Mixtral 8x7B Instruct via Ollama (quantized q4_K_M)
  category: general
  status: beta
  release_date: '2025-10-01'
  last_verified: '2025-11-10'
  recommended_for:
  - Knowledge base extraction
  - High-quality structured output
  not_recommended_for:
  - Real-time applications
  - Long context > 32k
  - Low-latency requirements
  last_benchmarked: '2025-11-12T14:06:51.891638'
requirements: {}
hardware_detection: {}

# === VAST.AI SERVERLESS CONFIG ===
vastai_config:
  enabled: true
  endpoint_name: "vLLM-Mixtral-8x7B"
  huggingface_model: "mistralai/Mixtral-8x7B-Instruct-v0.1"
  cold_workers: 0  # Cold start only - save costs
  skip_local: true  # 26GB MoE - too large for parallel workloads on Spark/Mac
  # NOTE: MoE weights are large; require ample disk for HF cache + image layers.
  disk_space_gb: 220
  gpu_requirements: "gpu_ram>=48 num_gpus=1 inet_down>100"
  template_hash: "03fba9bdccaa8ace9ad931105d8e9eb4"  # pragma: allowlist secret
  estimated_cold_start_seconds: 300
  priority: 5  # Quality MoE model

# === RESOURCE CLASS (for benchmarking) ===
resource_class:
  class: C  # High-Quality (12-18GB+ VRAM)
  vram_gb: 26.0
  parallel_capacity_spark: 4  # Max parallel on 128GB GPU
  recommended_parallel: 3
  throughput_tier: low
  tokens_per_second_single: 18

# === MODEL RANKING SYSTEM ===
tier: A
tier_rank: 4  # Quality A-tier MoE

# === BASE SCORES (from public benchmarks - DO NOT MODIFY) ===
base_scores:
  source: "Mistral AI Mixtral 8x7B Report, December 2023"
  source_url: "https://mistral.ai/news/mixtral-of-experts/"
  captured_date: "2025-12-05"
  raw_benchmarks:
    mmlu: 70.6
    gsm8k: 74.4
    arc_challenge: 68.5
    hellaswag: 84.4
    humaneval: 40.2
    ifeval: 75.2
    math: 52.1
  notes: "Mixtral 8x7B MoE - high quality with efficient inference"

# === CALIBRATED DIMENSION SCORES (0-100 normalized) ===
dimension_scores:
  summarization: 78
  reasoning: 76
  instruction: 78
  analysis: 75
  coding: 62
  speed: 40  # Large model, slower
  context_efficiency: 72

pom_benchmarks:
  last_calibration: "2025-12-05"
  calibration_version: "1.0"
  prompts_tested: {}
  aggregate:
    avg_quality: 76
    avg_latency_ms: 2500
    total_prompts_tested: 0

benchmarks:
  last_run: '2025-11-12T14:06:51.891626'
  tests_passed: 5/5
  summary:
    avg_latency_ms: 4122.6
    avg_tokens_per_second: 17.8
    total_time_seconds: 20.61
  tests:
    simple_instruction:
      latency_ms: 662.8
      tokens_per_second: 9.1
      time_seconds: 0.663
      estimated_tokens: 6.0
    structured_output:
      latency_ms: 2272.6
      tokens_per_second: 17.2
      time_seconds: 2.273
      estimated_tokens: 39.0
    reasoning:
      latency_ms: 8419.0
      tokens_per_second: 17.7
      time_seconds: 8.419
      estimated_tokens: 148.8
    summarization:
      latency_ms: 1454.2
      tokens_per_second: 22.0
      time_seconds: 1.454
      estimated_tokens: 32.0
    code_generation:
      latency_ms: 7804.2
      tokens_per_second: 22.9
      time_seconds: 7.804
      estimated_tokens: 178.8
