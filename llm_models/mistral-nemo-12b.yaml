id: mistral:nemo:12b
name: Mistral Nemo 12B Instruct
type: llm_model
version: 1.0.0
provider:
  name: ollama
  api_type: ollama
adapter_config:
  type: ollama
  model: mistral-nemo:12b-instruct-2407-q4_K_M
  base_url: null
  base_url_env: OLLAMA_HOST
parameters:
  temperature: 0.7
  max_tokens: 2000
  top_p: 0.9
capabilities:
  structured_output: true
  reasoning: true
  tool_calling: true
  streaming: true
  vision: false
  function_calling: true
  multimodal: false
cost:
  per_1m_input_tokens: 0.0
  per_1m_output_tokens: 0.0
  currency: USD
  notes: Free local inference on DGX Spark
performance:
  avg_latency_ms: 350
  tokens_per_second: 70
  context_window: 128000  # Mistral Nemo 12B supports 128K context

# Context variants for strategy-aware context management
context_variants:
  throughput: {context_window: 8192}   # Fast processing
  balanced: {context_window: 32768}    # Good balance for 12B model
  quality: {context_window: 65536}     # Extended for better quality
  extended: {context_window: 128000}   # Full 128K context

  backends:
    mac_metal:
      optimal_parallelism: 1
    spark_cuda:
      optimal_parallelism: 14
metadata:
  description: Mistral Nemo 12B Instruct - High-quality instruction-tuned model
  category: general
  status: stable
resource_class:
  class: C
  vram_gb: 0.0
  throughput_tier: moderate
