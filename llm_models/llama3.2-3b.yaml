id: "llama3.2:3b"
name: Llama 3.2 3B Instruct
type: llm_model
version: "1.0.0"
provider:
  name: ollama
  api_type: ollama
adapter_config:
  type: ollama
  model: llama3.2:3b-instruct-q4_K_M
  base_url: null
  base_url_env: null
parameters:
  temperature: 0.7
  max_tokens: 500
  top_p: 0.9
capabilities:
  structured_output: true
  reasoning: false
  tool_calling: false
  streaming: true
  vision: false
  function_calling: false
cost:
  per_1m_input_tokens: 0.0
  per_1m_output_tokens: 0.0
  currency: USD
  notes: Free local inference
performance:
  avg_latency_ms: 1255.7
  tokens_per_second: 99
  context_window: 8192  # Practical for high parallelism (theoretical: 131072)
metadata:
  description: Meta Llama 3.2 3B - Fastest model with 128K context
  category: general
  status: stable
  release_date: '2024-09-01'
  last_verified: '2025-11-05'
  recommended_for:
  - Fast local inference
  - Development/testing
  - Quick prototyping
  - Real-time applications
  - High-throughput tasks
  last_benchmarked: '2025-11-12T14:06:51.911222'
  not_recommended_for: []
requirements:
  gpu_vram_mb: 2000
  system_ram_mb: 3000
  cpu_cores: 2.0

hardware_detection:
  execution_mode: local_gpu
  optimal_concurrency: 12
  max_concurrency: 20
  dgx_spark:
    optimal_parallel_workers: 12
    recommended_batch_size: 1
    memory_usage_gb: 2.0
    estimated_throughput: 6.5
    tokens_per_second: 99
    notes: "Excellent throughput on GB10 with 128K context"
  gpu_24gb_plus:
    optimal_parallel_workers: 10
    recommended_batch_size: 2
  gpu_8gb_plus:
    optimal_parallel_workers: 6
    recommended_batch_size: 1
  cpu_only:
    optimal_parallel_workers: 4
    recommended_batch_size: 1

# === EXTERNAL MODEL CARDS (official sources) ===
external_card:
  huggingface: https://huggingface.co/meta-llama/Llama-3.2-3B-Instruct
  ollama: https://ollama.ai/library/llama3.2
  meta: https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2

# === RESOURCE CLASS (for benchmarking) ===
resource_class:
  class: A  # Ultra-Efficient (2-5GB VRAM)
  vram_gb: 2.0
  parallel_capacity_spark: 40  # Max parallel on 128GB GPU
  recommended_parallel: 12
  throughput_tier: high
  tokens_per_second_single: 99

# === MODEL RANKING SYSTEM ===
tier: C
tier_rank: 3  # Fast C-tier

# === BASE SCORES (from public benchmarks - DO NOT MODIFY) ===
base_scores:
  source: "Meta Llama 3.2 Model Card, September 2024"
  source_url: "https://www.llama.com/docs/model-cards-and-prompt-formats/llama3_2"
  captured_date: "2025-12-05"
  raw_benchmarks:
    mmlu: 58.5
    gsm8k: 45.2
    arc_challenge: 48.5
    hellaswag: 68.3
    humaneval: 28.0
    ifeval: 55.8
    math: 25.4
  notes: "Smallest Llama 3.2, optimized for speed and 128K context"

# === CALIBRATED DIMENSION SCORES (0-100 normalized) ===
dimension_scores:
  summarization: 62
  reasoning: 52
  instruction: 60
  analysis: 55
  coding: 45
  speed: 98  # Fastest model
  context_efficiency: 75

pom_benchmarks:
  last_calibration: "2025-12-05"
  calibration_version: "1.0"
  prompts_tested: {}
  aggregate:
    avg_quality: 55
    avg_latency_ms: 500
    total_prompts_tested: 0

benchmarks:
  last_run: '2025-11-12T14:06:51.911215'
  tests_passed: 5/5
  summary:
    avg_latency_ms: 1255.7
    avg_tokens_per_second: 59.2
    total_time_seconds: 6.28
  tests:
    simple_instruction:
      latency_ms: 187.3
      tokens_per_second: 8.0
      time_seconds: 0.187
      estimated_tokens: 1.5
    structured_output:
      latency_ms: 474.2
      tokens_per_second: 44.8
      time_seconds: 0.474
      estimated_tokens: 21.2
    reasoning:
      latency_ms: 2498.0
      tokens_per_second: 73.1
      time_seconds: 2.498
      estimated_tokens: 182.5
    summarization:
      latency_ms: 633.2
      tokens_per_second: 79.4
      time_seconds: 0.633
      estimated_tokens: 50.2
    code_generation:
      latency_ms: 2485.8
      tokens_per_second: 90.6
      time_seconds: 2.486
      estimated_tokens: 225.2
