id: "llama3.2:3b"
name: Llama 3.2 3B Instruct
type: llm_model
version: "1.0.0"
provider:
  name: ollama
  api_type: ollama
adapter_config:
  type: ollama
  model: llama3.2:3b-instruct-q4_K_M
  base_url: null
  base_url_env: null
parameters:
  temperature: 0.7
  max_tokens: 500
  top_p: 0.9
capabilities:
  structured_output: true
  reasoning: false
  tool_calling: false
  streaming: true
  vision: false
  function_calling: false
cost:
  per_1m_input_tokens: 0.0
  per_1m_output_tokens: 0.0
  currency: USD
  notes: Free local inference
performance:
  backends:
    mac_metal:
      optimal_parallelism: 1  # Mac Ollama does NOT handle parallel - always use 1
    spark_cuda:
      optimal_parallelism: 12  # Spark optimal parallel setting