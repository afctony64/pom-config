id: "qwen3:32b"
name: Qwen 3 32B
type: llm_model
version: "1.0.0"
provider: {name: ollama, api_type: ollama}
adapter_config: {type: ollama, model: "qwen3:32b", base_url: null, base_url_env: OLLAMA_HOST}
parameters: {temperature: 0.7, max_tokens: 4000, top_p: 0.9}
capabilities: {structured_output: true, reasoning: true, tool_calling: true, streaming: true, vision: false, function_calling: true, multimodal: false}
cost: {per_1m_input_tokens: 0.0, per_1m_output_tokens: 0.0, currency: USD, notes: "Free local"}
performance: {avg_latency_ms: 3000, tokens_per_second: 22, context_window: 24576}  # Practical (theoretical: 131072)
metadata: {description: "Alibaba Qwen 3 32B - Best reasoning", category: reasoning, status: stable, release_date: "2025-04-01", last_verified: "2025-12-05", family: qwen3, family_tier: premium, recommended_for: ["Complex reasoning", "Coding"], not_recommended_for: ["Simple tasks"]}
requirements: {gpu_vram_mb: 20200, system_ram_mb: 24000, cpu_cores: 4.0}
hardware_detection: {execution_mode: local_gpu, optimal_concurrency: 4, max_concurrency: 6, dgx_spark: {optimal_parallel_workers: 5, recommended_batch_size: 1, memory_usage_gb: 20.2, tokens_per_second: 22}}
resource_class: {class: C, vram_gb: 20.2, parallel_capacity_spark: 6, recommended_parallel: 5, throughput_tier: low, tokens_per_second_single: 22}
tier: S
tier_rank: 2
dimension_scores: {summarization: 85, reasoning: 94, instruction: 88, analysis: 90, coding: 95, speed: 28, context_efficiency: 90}
pom_benchmarks: {last_calibration: "2025-12-05", calibration_version: "2.0", aggregate: {avg_quality: 90, avg_latency_ms: 3000, total_prompts_tested: 0}}
benchmarks: {}

# === VAST.AI SERVERLESS CONFIGURATION ===
vastai_config:
  enabled: true
  endpoint_name: "vLLM-Qwen3-32B"
  huggingface_model: "Qwen/Qwen3-32B"
  gpu_tier: "professional"  # A6000/L40 - needs 48GB+
  skip_local: true  # 20GB - skip Mac fallback for parallel workloads
  cold_workers: 0  # Cold start only - save costs
  # NOTE: Large weights; require ample disk for HF cache + image layers.
  disk_space_gb: 220
  gpu_requirements: "gpu_ram>=48 num_gpus=1 inet_down>100"
  template_hash: "03fba9bdccaa8ace9ad931105d8e9eb4"  # pragma: allowlist secret
  estimated_cold_start_seconds: 300  # 5 min cold start for large model
  priority: 5  # Lower priority - expensive
