id: "minimax-m2"
name: MiniMax M2
type: llm_model
version: "1.0.0"
provider: {name: ollama, api_type: ollama}
adapter_config: {type: ollama, model: "minimax-m2:cloud", base_url: null, base_url_env: OLLAMA_HOST}
parameters: {temperature: 0.7, max_tokens: 4000, top_p: 0.9}
capabilities: {structured_output: true, reasoning: false, tool_calling: true, streaming: true, vision: false, function_calling: true, multimodal: false}
cost: {per_1m_input_tokens: 0.0, per_1m_output_tokens: 0.0, currency: USD, notes: "Free VastAI"}
performance: {avg_latency_ms: 1500, tokens_per_second: 35, context_window: 32768}

# Context variants for strategy-aware context management (VastAI cloud)
context_variants:
  throughput: {context_window: 8192}   # Fast processing
  balanced: {context_window: 16384}    # Good balance
  quality: {context_window: 32768}     # Full context for best quality
metadata:
  description: "MiniMax M2 - High-quality multilingual model"
  category: general
  status: stable
  release_date: "2025-02-01"
  last_verified: "2025-12-15"
  family: minimax
  family_tier: premium
  strength: multilingual
  recommended_for:
    - Multilingual tasks
    - Long-form generation
    - Creative writing
  not_recommended_for:
    - Code generation
requirements: {gpu_vram_mb: 40000, system_ram_mb: 48000, cpu_cores: 8.0}
hardware_detection:
  execution_mode: cloud_gpu
  optimal_concurrency: 4
  max_concurrency: 6
vastai_config:
  enabled: true
  endpoint_name: "vLLM-MiniMax-M2"
  huggingface_model: "MiniMaxAI/MiniMax-M2"
  cold_workers: 0
  # NOTE: Large model class; require ample disk for HF cache + image layers.
  disk_space_gb: 220
  gpu_requirements: "gpu_ram>=80 num_gpus=1 inet_down>100"
  template_hash: "03fba9bdccaa8ace9ad931105d8e9eb4"
  estimated_cold_start_seconds: 300
  priority: 4
  skip_local: true  # Too large for local Ollama
resource_class: {class: C, vram_gb: 40.0, parallel_capacity_spark: 3, recommended_parallel: 2, throughput_tier: low, tokens_per_second_single: 35}
tier: A
tier_rank: 2
dimension_scores:
  summarization: 85
  reasoning: 82
  instruction: 85
  analysis: 80
  coding: 60
  speed: 45
  context_efficiency: 88
pom_benchmarks:
  last_calibration: "2025-12-15"
  calibration_version: "1.0"
  aggregate: {avg_quality: 82, avg_latency_ms: 1500, total_prompts_tested: 0}
benchmarks: {}
