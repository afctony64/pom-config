id: "gemma3:1b"
name: Gemma 3 1B
type: llm_model
version: "1.0.0"

provider:
  name: ollama
  api_type: ollama

adapter_config:
  type: ollama
  model: gemma3:1b
  base_url: null
  base_url_env: null

parameters:
  temperature: 0.7
  max_tokens: 500
  top_p: 0.9

capabilities:
  structured_output: true
  reasoning: false
  tool_calling: false
  streaming: true
  vision: false  # 1B does NOT have vision
  function_calling: false
  multimodal: false

cost:
  per_1m_input_tokens: 0.0
  per_1m_output_tokens: 0.0
  currency: USD
  notes: "Free local inference"

performance:
  avg_latency_ms: 0  # To be benchmarked
  tokens_per_second: 0  # To be benchmarked
  context_window: 32000  # 32K context for 1B model

metadata:
  description: "Google Gemma 3 1B - Ultra-lightweight model for edge deployment"
  category: general
  status: stable
  release_date: "2025-03-01"
  last_verified: "2025-12-05"
  family: gemma3
  family_tier: entry
  recommended_for:
    - "Edge deployment"
    - "Resource-constrained environments"
    - "Simple text tasks"
    - "Fast inference needs"
    - "Mobile/embedded applications"
  not_recommended_for:
    - "Complex reasoning"
    - "Long document analysis"
    - "High-quality text generation"
    - "Vision tasks (use 4B+)"

requirements:
  gpu_vram_mb: 1000  # ~1GB for Q4 quantized
  system_ram_mb: 2000
  cpu_cores: 1.0

hardware_detection:
  execution_mode: local_gpu
  optimal_concurrency: 20
  max_concurrency: 50
  dgx_spark:
    optimal_parallel_workers: 50
    recommended_batch_size: 1
    memory_usage_gb: 0.8
    estimated_throughput: 2.0
    tokens_per_second: 80  # Estimated - fast due to small size
    notes: "Ultra-fast on Spark - can run many parallel instances"
  gpu_24gb_plus:
    optimal_parallel_workers: 20
    recommended_batch_size: 1
  gpu_8gb_plus:
    optimal_parallel_workers: 8
    recommended_batch_size: 1
  cpu_only:
    optimal_parallel_workers: 4
    recommended_batch_size: 1

resource_class:
  class: A  # Lightweight (<6GB VRAM)
  vram_gb: 0.8
  parallel_capacity_spark: 100  # Can run 100+ parallel on 128GB
  recommended_parallel: 50
  throughput_tier: high
  tokens_per_second_single: 80

# === MODEL RANKING SYSTEM ===
tier: C
tier_rank: 2

# === BASE SCORES (from public benchmarks - DO NOT MODIFY) ===
base_scores:
  source: "Google Gemma 3 Technical Report, March 2025"
  source_url: "https://ai.google.dev/gemma/docs"
  captured_date: "2025-12-05"
  raw_benchmarks:
    mmlu: 39.2
    gsm8k: 28.4
    arc_challenge: 35.1
    hellaswag: 52.6
    humaneval: 12.8
    ifeval: 42.3
    math: 15.2
  notes: "Smallest model - optimized for speed over quality"

# === CALIBRATED DIMENSION SCORES (0-100 normalized) ===
dimension_scores:
  summarization: 55
  reasoning: 45
  instruction: 55
  analysis: 50
  coding: 35
  speed: 95
  context_efficiency: 60

pom_benchmarks:
  last_calibration: "2025-12-05"
  calibration_version: "1.0"
  prompts_tested:
    research_synopsis:
      quality_score: 52
      latency_ms: 400
      tokens_generated: 280
      cost_per_run: 0.0
    researcher_synopsis:
      quality_score: 55
      latency_ms: 250
      tokens_generated: 120
      cost_per_run: 0.0
    analyze_table:
      quality_score: 50
      latency_ms: 150
      tokens_generated: 80
      cost_per_run: 0.0
  aggregate:
    avg_quality: 52
    avg_latency_ms: 267
    total_prompts_tested: 3

benchmarks: {}
