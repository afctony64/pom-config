id: "gpt-oss:120b"
name: GPT-OSS 120B
type: llm_model
version: "1.0.0"
provider: {name: ollama, api_type: ollama}
adapter_config: {type: ollama, model: "gpt-oss:120b", base_url: null, base_url_env: OLLAMA_HOST}
parameters: {temperature: 0.7, max_tokens: 8000, top_p: 0.9}
capabilities: {structured_output: true, reasoning: true, tool_calling: false, streaming: true, vision: false, function_calling: false, multimodal: false}
cost: {per_1m_input_tokens: 0.0, per_1m_output_tokens: 0.0, currency: USD, notes: "Free local"}
performance: {avg_latency_ms: 8000, tokens_per_second: 12, context_window: 32000}
metadata: {description: "GPT-OSS 120B - Massive open source", category: general, status: stable, release_date: "2025-01-01", last_verified: "2025-12-05", family: gpt-oss, family_tier: premium, recommended_for: ["Maximum quality"], not_recommended_for: ["Speed-sensitive"]}
requirements: {gpu_vram_mb: 65400, system_ram_mb: 70000, cpu_cores: 8.0}
hardware_detection: {execution_mode: local_gpu, optimal_concurrency: 1, max_concurrency: 2, dgx_spark: {optimal_parallel_workers: 1, recommended_batch_size: 1, memory_usage_gb: 65.4, tokens_per_second: 12}}
vastai_config:
  enabled: true
  endpoint_name: "vLLM-GPT-OSS-120B"
  huggingface_model: "meta-llama/Llama-3.3-70B-Instruct"  # Using Llama 3.3 70B as proxy
  skip_local: true  # 65GB - too large for Mac/Spark
  cold_workers: 0  # Free tier - large model
  # NOTE: 70B+ class weights are very large; require ample disk for HF cache + image layers.
  disk_space_gb: 250
  gpu_requirements: "gpu_ram>=80 num_gpus=1 inet_down>100"
  template_hash: "03fba9bdccaa8ace9ad931105d8e9eb4"  # pragma: allowlist secret
  estimated_cold_start_seconds: 300
  priority: 3  # Lower priority - expensive H100 required

resource_class: {class: C, vram_gb: 65.4, parallel_capacity_spark: 1, recommended_parallel: 1, throughput_tier: low, tokens_per_second_single: 12}
tier: S
tier_rank: 1
dimension_scores: {summarization: 94, reasoning: 96, instruction: 92, analysis: 95, coding: 90, speed: 12, context_efficiency: 75}
pom_benchmarks: {last_calibration: "2025-12-05", calibration_version: "2.0", aggregate: {avg_quality: 94, avg_latency_ms: 8000, total_prompts_tested: 0}}
benchmarks: {}
