id: "gpt-oss:120b"
aliases:
  - "gpt-oss-120b"
name: GPT-OSS 120B
type: llm_model
version: "1.0.0"

# Now using Groq for cloud inference (was VastAI)
provider:
  name: groq
  api_type: chat

adapter_config:
  type: groq
  model: openai/gpt-oss-120b
  base_url: https://api.groq.com/openai/v1
  base_url_env: GROQ_API_KEY

parameters:
  temperature: 0.7
  max_tokens: 8000
  top_p: 0.9

capabilities:
  structured_output: true
  reasoning: true  # This is a reasoning model
  tool_calling: false
  streaming: true
  vision: false
  function_calling: false
  multimodal: false

cost:
  per_1m_input_tokens: 0.59
  per_1m_output_tokens: 0.99
  currency: USD
  notes: "Groq LPU inference - reasoning model"

performance:
  avg_latency_ms: 300
  tokens_per_second: 300
  context_window: 131072

metadata:
  description: "GPT-OSS 120B reasoning model on Groq LPU - uses reasoning field for output"
  category: reasoning
  status: stable
  release_date: "2025-01-01"
  last_verified: "2025-12-18"
  family: gpt-oss
  family_tier: premium
  recommended_for:
    - "Maximum quality reasoning"
    - "Complex analysis"
  not_recommended_for:
    - "Simple tasks"
    - "Speed-sensitive applications"

requirements:
  gpu_vram_mb: 0  # Cloud-only
  system_ram_mb: 0
  cpu_cores: 0

hardware_detection:
  execution_mode: cloud

routing: [groq]

resource_class:
  class: A
  vram_gb: 0
  throughput_tier: high
  tokens_per_second_single: 300

tier: S
tier_rank: 1

dimension_scores:
  summarization: 94
  reasoning: 96
  instruction: 92
  analysis: 95
  coding: 90
  speed: 80
  context_efficiency: 75

pom_benchmarks:
  last_calibration: "2025-12-18"
  calibration_version: "2.0"
  aggregate:
    avg_quality: 94
    avg_latency_ms: 300
    total_prompts_tested: 0

benchmarks: {}
