id: "groq-llama4-maverick"
aliases:
  - "llama4-maverick"
  - "llama-4-maverick"
  - "groq:llama4-maverick"
name: Groq Llama 4 Maverick 17Bx128E
type: llm_model
version: "1.0.0"

provider:
  name: groq
  api_type: chat

adapter_config:
  type: groq
  model: meta-llama/llama-4-maverick-17b-128e-instruct
  base_url: https://api.groq.com/openai/v1
  base_url_env: GROQ_API_KEY

parameters:
  temperature: 0.7
  max_tokens: 4000
  top_p: 0.9

capabilities:
  structured_output: true
  reasoning: true
  tool_calling: true
  streaming: true
  vision: false
  function_calling: true
  multimodal: false

cost:
  per_1m_input_tokens: 0.20
  per_1m_output_tokens: 0.60
  currency: USD
  notes: "Groq LPU - Llama 4 Maverick larger/smarter"

performance:
  avg_latency_ms: 220
  tokens_per_second: 562
  context_window: 131072

metadata:
  description: "Llama 4 Maverick - larger expert count, higher quality"
  category: general
  status: stable
  release_date: "2025-04-01"
  last_verified: "2025-12-18"
  family: llama4
  family_tier: premium
  recommended_for:
    - "Complex reasoning"
    - "High quality responses"
    - "Multi-step tasks"
  not_recommended_for:
    - "Maximum cost efficiency"

requirements:
  gpu_vram_mb: 0
  system_ram_mb: 0
  cpu_cores: 0

hardware_detection:
  execution_mode: cloud

routing: [groq]

resource_class:
  class: A
  vram_gb: 0
  throughput_tier: high
  tokens_per_second_single: 562

tier: S
tier_rank: 1

dimension_scores:
  summarization: 92
  reasoning: 90
  instruction: 92
  analysis: 90
  coding: 88
  speed: 95
  context_efficiency: 92

pom_benchmarks:
  last_calibration: "2025-12-18"
  calibration_version: "2.0"
  aggregate:
    avg_quality: 90
    avg_latency_ms: 220
    total_prompts_tested: 0

benchmarks: {}
