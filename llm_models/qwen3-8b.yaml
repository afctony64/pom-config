id: "qwen3:8b"
name: Qwen 3 8B
type: llm_model
version: "1.0.0"

provider:
  name: ollama
  api_type: ollama

adapter_config:
  type: ollama
  model: qwen3:8b
  base_url: null
  base_url_env: null

parameters:
  temperature: 0.1
  max_tokens: 1000
  top_p: 0.9

capabilities:
  structured_output: true
  reasoning: false
  tool_calling: false
  streaming: true
  vision: false
  function_calling: false

cost:
  per_1m_input_tokens: 0.0
  per_1m_output_tokens: 0.0
  currency: USD
  notes: "Free local inference"

performance:
  avg_latency_ms: 200
  tokens_per_second: 85  # Mac Metal average
  context_window: 16384  # Practical for Spark (theoretical: 128K)
  context_window_max: 131072  # Extended via YaRN/RoPE scaling (model-dependent)
  
  # Backend-specific benchmarks (December 2025)
  backends:
    mac_metal:
      tokens_per_second: 85
      avg_latency_ms: 200
      optimal_parallelism: 1  # Mac Ollama does NOT handle parallel - always use 1
      memory_bandwidth_gbs: 546
    spark_cuda:
      tokens_per_second: 117  # Parallel mode (6 parallel) - from parallelism benchmark Dec 19
      tokens_per_second_single: 25  # Single mode - from parallelism benchmark Dec 19
      avg_latency_ms: 500
      optimal_parallelism: 6  # Spark handles parallel well - optimal is 6 for qwen3:8b
      memory_bandwidth_gbs: 273
      notes: "Parallel performance (117 tok/s) is 4.8x faster than single (25 tok/s). Performance degrades with long requests (>100 tokens)."
    groq:
      tokens_per_second: 500
      avg_latency_ms: 80
      model_id: "qwen/qwen3-8b"

# ==============================================================================
# PRIORITY CONFIGURATION
# ==============================================================================
# PomSpark routes requests based on priority to maximize efficiency.
# Apps can specify priority, or it's inferred from task type.

priority_config:
  default: fast                   # Default priority when not specified
  supports: [ultra, fast, batch, max_throughput]
  groq_equivalent: "qwen/qwen3-32b"  # Groq only has 32b, map 8b to 32b
  
  # Task-specific priority overrides
  # When request metadata matches these patterns, use the specified priority
  task_priorities:
    # User-facing, latency-critical
    chat: fast
    conversation: fast
    assistant: fast
    
    # Background processing, accuracy over speed
    fact_extraction: batch
    sanitization: batch
    analysis: batch
    summarization: batch
    research: batch
    
    # Real-time, instant response needed
    autocomplete: ultra
    suggestions: ultra
    quick_response: ultra
    
    # High volume, maximize throughput
    batch_processing: max_throughput
    bulk_generation: max_throughput
    embedding_generation: max_throughput

# Routing configuration
# Issue #72: Updated routing to prefer Groq for parallel processing (prevents timeout failures)
# Ollama is SERIAL - only 1 request at a time. Groq handles parallel requests natively.
routing:
  preferred_backend: groq   # Prefer Groq for parallel capability and speed
  fallback_backends: [spark, mac]  # Fall back to Spark CUDA, then Mac Metal
  parallel_capable: true    # This model supports parallel processing via Groq
  serial_backends: [mac, spark]  # These are serial - avoid for parallel workloads
routing_priority:
  ultra: groq              # Instant response via Groq LPU (~500 tok/s)
  urgent: groq             # Issue #72: Route urgent to Groq for reliability
  fast: groq               # Issue #72: Route fast to Groq (was mac - caused serial bottleneck)
  chat: groq               # Issue #72: Chat needs parallel capability
  batch: spark             # Background jobs via Spark CUDA - single stream OK
  max_throughput: groq     # Issue #72: Groq for max throughput, not serial ollama-lb

metadata:
  description: "Alibaba Qwen 3 8B - Current default for Fact Sanitizer"
  category: general
  status: stable
  release_date: "2025-11-19"
  last_verified: "2025-11-19"
  recommended_for:
    - "Fact extraction and sanitization"
    - "Deterministic output (low temperature)"
    - "General purpose tasks"
    - "Multilingual support"
  not_recommended_for:
    - "Real-time applications"
    - "Creative tasks"

requirements:
  gpu_vram_mb: 6000  # 8B model needs ~6GB VRAM (Q4 quantized)
  system_ram_mb: 4000
  cpu_cores: 2.0

hardware_detection:
  execution_mode: local_gpu
  optimal_concurrency: 6  # For Spark only - from parallelism benchmark Dec 19
  max_concurrency: 12  # For Spark only
  mac_metal:
    optimal_parallel_workers: 1  # Mac Ollama does NOT handle parallel - always 1
    recommended_batch_size: 1
    notes: "Mac processes requests sequentially - parallel requests queue"
  dgx_spark:
    optimal_parallel_workers: 6  # Spark handles parallel well - from parallelism benchmark Dec 19
    recommended_batch_size: 1
    memory_usage_gb: 5.2
    estimated_throughput: 0.5
    tokens_per_second: 25
    notes: "Good balance of quality and speed on GB10"
  gpu_24gb_plus:
    optimal_parallel_workers: 4
    recommended_batch_size: 1
  gpu_8gb_plus:
    optimal_parallel_workers: 2
    recommended_batch_size: 1
  cpu_only:
    optimal_parallel_workers: 1
    recommended_batch_size: 1

  gpu_requirements: "gpu_ram>=20 num_gpus=1 inet_down>100"
  template_hash: "03fba9bdccaa8ace9ad931105d8e9eb4"  # vLLM Serverless template  # pragma: allowlist secret
  estimated_cold_start_seconds: 180  # Cold start time
  priority: 10  # High priority - primary cloud fallback

# === RESOURCE CLASS (for benchmarking) ===
resource_class:
  class: B  # Balanced (6-11GB VRAM)
  vram_gb: 5.2
  parallel_capacity_spark: 6  # Optimal parallel from benchmark Dec 19 (NOT 20 - Spark cannot handle that many)
  recommended_parallel: 6  # For Spark only - from parallelism benchmark Dec 19
  recommended_parallel_mac: 1  # Mac always uses 1 (doesn't handle parallel)
  throughput_tier: moderate
  tokens_per_second_single: 25

# === MODEL RANKING SYSTEM ===
tier: C
tier_rank: 1  # Best in C tier

# Named context variants (local defaults should stay conservative for throughput)
context_variants:
  throughput: {context_window: 8192}
  balanced: {context_window: 16384}
  quality: {context_window: 32768}
  extended: {context_window: 131072, notes: "Extended context; slower, requires proper scaling support"}

# === BASE SCORES (from public benchmarks - DO NOT MODIFY) ===
base_scores:
  source: "Alibaba Qwen 3 Technical Report, November 2025"
  source_url: "https://qwenlm.github.io/blog/qwen3/"
  captured_date: "2025-12-05"
  raw_benchmarks:
    mmlu: 65.3
    gsm8k: 58.2
    arc_challenge: 58.4
    hellaswag: 72.8
    humaneval: 38.4
    ifeval: 64.2
    math: 42.1
  notes: "Good for fact extraction and deterministic tasks"

# === CALIBRATED DIMENSION SCORES (0-100 normalized) ===
dimension_scores:
  summarization: 68
  reasoning: 62
  instruction: 68
  analysis: 65
  coding: 58
  speed: 85
  context_efficiency: 75

pom_benchmarks:
  last_calibration: "2025-12-05"
  calibration_version: "1.0"
  prompts_tested:
    research_synopsis:
      quality_score: 66
      latency_ms: 1000
      tokens_generated: 300
      cost_per_run: 0.0
    researcher_synopsis:
      quality_score: 68
      latency_ms: 600
      tokens_generated: 130
      cost_per_run: 0.0
    analyze_table:
      quality_score: 65
      latency_ms: 350
      tokens_generated: 85
      cost_per_run: 0.0
  aggregate:
    avg_quality: 66
    avg_latency_ms: 650
    total_prompts_tested: 3

benchmarks: {}
