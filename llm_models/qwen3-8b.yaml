id: "qwen3:8b"
name: Qwen 3 8B
type: llm_model
version: "1.0.0"

provider:
  name: ollama
  api_type: ollama

adapter_config:
  type: ollama
  model: qwen3:8b
  base_url: null
  base_url_env: null

parameters:
  temperature: 0.1
  max_tokens: 1000
  top_p: 0.9

capabilities:
  structured_output: true
  reasoning: false
  tool_calling: false
  streaming: true
  vision: false
  function_calling: false

cost:
  per_1m_input_tokens: 0.0
  per_1m_output_tokens: 0.0
  currency: USD
  notes: "Free local inference"

performance:
  avg_latency_ms: 2000
  tokens_per_second: 64  # Mac single-stream peak
  context_window: 16384  # Practical for Spark (theoretical: 128K)

metadata:
  description: "Alibaba Qwen 3 8B - Current default for Fact Sanitizer"
  category: general
  status: stable
  release_date: "2025-11-19"
  last_verified: "2025-11-19"
  recommended_for:
    - "Fact extraction and sanitization"
    - "Deterministic output (low temperature)"
    - "General purpose tasks"
    - "Multilingual support"
  not_recommended_for:
    - "Real-time applications"
    - "Creative tasks"

requirements:
  gpu_vram_mb: 6000  # 8B model needs ~6GB VRAM (Q4 quantized)
  system_ram_mb: 4000
  cpu_cores: 2.0

hardware_detection:
  execution_mode: local_gpu
  optimal_concurrency: 6
  max_concurrency: 12
  dgx_spark:
    optimal_parallel_workers: 6
    recommended_batch_size: 1
    memory_usage_gb: 5.2
    estimated_throughput: 0.5
    tokens_per_second: 64  # Mac single-stream peak
    notes: "Good balance of quality and speed on GB10"
  gpu_24gb_plus:
    optimal_parallel_workers: 4
    recommended_batch_size: 1
  gpu_8gb_plus:
    optimal_parallel_workers: 2
    recommended_batch_size: 1
  cpu_only:
    optimal_parallel_workers: 1
    recommended_batch_size: 1

# === VAST.AI SERVERLESS CONFIGURATION ===
# Cloud GPU burst capacity via Vast.ai Serverless
# Endpoint: https://cloud.vast.ai → Serverless → vLLM-Qwen3-8B
vastai_config:
  enabled: true
  endpoint_name: "vLLM-Qwen3-8B"
  huggingface_model: "Qwen/Qwen3-8B"
  cold_workers: 0  # Cold start only - save costs
  # NOTE: Disk must be generous on Vast.ai to avoid HF/vLLM download failures.
  disk_space_gb: 100
  gpu_requirements: "gpu_ram>=20 num_gpus=1 inet_down>100"
  template_hash: "03fba9bdccaa8ace9ad931105d8e9eb4"  # vLLM Serverless template  # pragma: allowlist secret
  estimated_cold_start_seconds: 180  # Cold start time
  priority: 10  # High priority - primary cloud fallback

# === RESOURCE CLASS (for benchmarking) ===
resource_class:
  class: B  # Balanced (6-11GB VRAM)
  vram_gb: 5.2
  parallel_capacity_spark: 16  # Max parallel on 128GB unified memory
  parallel_capacity_mac: 1    # Mac optimal (no parallelism benefit)
  recommended_parallel: 16
  throughput_tier: moderate
  tokens_per_second_single: 64
  tokens_per_second_spark: 260  # Spark at 16 parallel

# === MODEL RANKING SYSTEM ===
tier: C
tier_rank: 1  # Best in C tier

# === BASE SCORES (from public benchmarks - DO NOT MODIFY) ===
base_scores:
  source: "Alibaba Qwen 3 Technical Report, November 2025"
  source_url: "https://qwenlm.github.io/blog/qwen3/"
  captured_date: "2025-12-05"
  raw_benchmarks:
    mmlu: 65.3
    gsm8k: 58.2
    arc_challenge: 58.4
    hellaswag: 72.8
    humaneval: 38.4
    ifeval: 64.2
    math: 42.1
  notes: "Good for fact extraction and deterministic tasks"

# === CALIBRATED DIMENSION SCORES (0-100 normalized) ===
dimension_scores:
  summarization: 68
  reasoning: 62
  instruction: 68
  analysis: 65
  coding: 58
  speed: 85
  context_efficiency: 75

pom_benchmarks:
  last_calibration: "2025-12-05"
  calibration_version: "1.0"
  prompts_tested:
    research_synopsis:
      quality_score: 66
      latency_ms: 1000
      tokens_generated: 300
      cost_per_run: 0.0
    researcher_synopsis:
      quality_score: 68
      latency_ms: 600
      tokens_generated: 130
      cost_per_run: 0.0
    analyze_table:
      quality_score: 65
      latency_ms: 350
      tokens_generated: 85
      cost_per_run: 0.0
  aggregate:
    avg_quality: 66
    avg_latency_ms: 650
    total_prompts_tested: 3

benchmarks: {}
