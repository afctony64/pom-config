id: qwen3:8b
aliases:
  - qwen3-8b
name: Qwen 3 8B
type: llm_model
version: 1.0.0
provider:
  name: ollama
  api_type: ollama
adapter_config:
  type: ollama
  model: qwen3:8b
  base_url: null
  base_url_env: OLLAMA_HOST
parameters:
  temperature: 0.7
  max_tokens: 2000
  top_p: 0.9
capabilities:
  structured_output: true
  reasoning: false
  tool_calling: true
  streaming: true
  vision: false
  function_calling: true
  multimodal: false
cost:
  per_1m_input_tokens: 0.0
  per_1m_output_tokens: 0.0
  currency: USD
  notes: Free local inference on DGX Spark
performance:
  avg_latency_ms: 0  # Not published for Ollama distributions
  tokens_per_second: 0  # Not published for Ollama distributions
  context_window: 32768  # Native context length (Qwen3-8B model card)
context_variants:
  throughput: {context_window: 8192}
  balanced: {context_window: 16384}
  quality: {context_window: 32768}
  extended: {context_window: 131072}  # YaRN extended context (Qwen3-8B model card)
metadata:
  description: Qwen 3 8B (Ollama Q4_K_M). Native 32K context, 131K with YaRN.
  category: reasoning
  status: stable
  release_date: '2025-04-01'
  last_verified: '2025-12-22'
  family: qwen3
  family_tier: balanced
  strength: reasoning
  model_size_gb: 5.2  # Ollama listing
  parameters_b: 8.19  # Ollama listing
  quantization: Q4_K_M  # Ollama listing
  recommended_for:
    - Reasoning tasks
    - Tool calling workflows
  not_recommended_for:
    - Maximum reasoning quality (use 32b on Groq)
  notes: Ollama default context length is configurable; native model supports 32K.
requirements:
  gpu_vram_mb: 5200  # Model size from Ollama listing
  system_ram_mb: 8000
  cpu_cores: 4.0
hardware_detection:
  execution_mode: local_gpu
  optimal_concurrency: 8
  max_concurrency: 12
routing:
  - spark
  - mac
resource_class:
  class: C
  vram_gb: 5.2
  parallel_capacity_spark: 12
  recommended_parallel: 8
  throughput_tier: moderate
  tokens_per_second_single: 0  # Not published for Ollama distributions
tier: A
tier_rank: 2
dimension_scores:
  summarization: 78
  reasoning: 84
  instruction: 80
  analysis: 82
  coding: 84
  speed: 55
  context_efficiency: 86
pom_benchmarks:
  last_calibration: '2025-12-07'
  calibration_version: '3.0'
  prompts_tested: {}
  aggregate:
    avg_quality: 82
    avg_latency_ms: 0
    total_prompts_tested: 0
benchmarks: {}
