id: "gemma3:27b"
name: Gemma 3 27B IT-QAT
type: llm_model
version: "1.0.0"
provider: {name: ollama, api_type: ollama}
adapter_config: {type: ollama, model: "gemma3:27b-it-qat", base_url: null, base_url_env: OLLAMA_HOST}
parameters: {temperature: 0.7, max_tokens: 4000, top_p: 0.9}
capabilities: {structured_output: true, reasoning: true, tool_calling: false, streaming: true, vision: true, function_calling: false, multimodal: true}
cost: {per_1m_input_tokens: 0.0, per_1m_output_tokens: 0.0, currency: USD, notes: "Free local"}
performance: {avg_latency_ms: 2800, tokens_per_second: 25, context_window: 24576}  # Practical (theoretical: 128K)
metadata: {description: "Google Gemma 3 27B IT-QAT - Best summarization", category: general, status: stable, release_date: "2025-03-01", last_verified: "2025-12-05", family: gemma3, family_tier: premium, recommended_for: ["Executive summaries", "Dossiers"], not_recommended_for: ["Batch processing"]}
requirements: {gpu_vram_mb: 18100, system_ram_mb: 20000, cpu_cores: 4.0}
hardware_detection: {execution_mode: local_gpu, optimal_concurrency: 4, max_concurrency: 6, dgx_spark: {optimal_parallel_workers: 6, recommended_batch_size: 1, memory_usage_gb: 18.1, tokens_per_second: 25}}
vastai_config:
  enabled: true
  endpoint_name: "vLLM-Gemma3-27B"
  huggingface_model: "google/gemma-3-27b-it"
  cold_workers: 0  # Cold start only - save costs
  skip_local: true  # 18GB - skip Mac fallback for parallel workloads
  # NOTE: Larger weights + HF cache; avoid low-disk hosts to prevent download failures.
  disk_space_gb: 160
  gpu_requirements: "gpu_ram>=40 num_gpus=1 inet_down>100"
  template_hash: "03fba9bdccaa8ace9ad931105d8e9eb4"  # pragma: allowlist secret
  estimated_cold_start_seconds: 240
  priority: 5  # Lower priority - large model

resource_class: {class: C, vram_gb: 18.1, parallel_capacity_spark: 7, recommended_parallel: 6, throughput_tier: low, tokens_per_second_single: 25}
tier: S
tier_rank: 3
dimension_scores: {summarization: 92, reasoning: 88, instruction: 90, analysis: 88, coding: 75, speed: 30, context_efficiency: 92}
pom_benchmarks: {last_calibration: "2025-12-05", calibration_version: "2.0", aggregate: {avg_quality: 90, avg_latency_ms: 2800, total_prompts_tested: 0}}
benchmarks: {}
