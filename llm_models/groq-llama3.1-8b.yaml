id: groq-llama3.1-8b
aliases:
- groq:llama3.1-8b
name: Groq Llama 3.1 8B Instant
type: llm_model
version: 1.0.0
provider:
  name: groq
  api_type: chat
adapter_config:
  type: groq
  model: llama-3.1-8b-instant
  base_url: https://api.groq.com/openai/v1
  base_url_env: GROQ_API_KEY
parameters:
  temperature: 0.7
  max_tokens: 4000
  top_p: 0.9
capabilities:
  structured_output: true
  reasoning: false
  tool_calling: true
  streaming: true
  vision: false
  function_calling: true
  multimodal: false
cost:
  per_1m_input_tokens: 0.05
  per_1m_output_tokens: 0.08
  currency: USD
  notes: Groq LPU - cheapest, fastest small model
performance:
  avg_latency_ms: 150
  tokens_per_second: 840
  context_window: 131072

# Context variants for strategy-aware context management (Groq - ultra-fast small model)
context_variants:
  throughput: {context_window: 8192}    # Ultra-fast, minimal cost
  balanced: {context_window: 16384}     # Good balance for 8B
  quality: {context_window: 32768}      # Extended
  extended: {context_window: 131072}    # Full 131K context
metadata:
  description: Llama 3.1 8B on Groq - ultra cheap and fast
  category: general
  status: stable
  release_date: '2024-07-23'
  last_verified: '2025-12-18'
  family: llama3.1
  family_tier: standard
  recommended_for:
  - Quick tasks
  - High volume
  - Cost-sensitive
  not_recommended_for:
  - Complex reasoning
requirements:
  gpu_vram_mb: 0
  system_ram_mb: 0
  cpu_cores: 0
hardware_detection:
  execution_mode: cloud
routing:
- groq
resource_class:
  class: B
  vram_gb: 0
  throughput_tier: high
  tokens_per_second_single: 840
tier: B
tier_rank: 3
dimension_scores:
  summarization: 78
  reasoning: 72
  instruction: 80
  analysis: 75
  coding: 70
  speed: 99
  context_efficiency: 90
pom_benchmarks:
  last_calibration: '2025-12-18'
  calibration_version: '2.0'
  aggregate:
    avg_quality: 75
    avg_latency_ms: 150
    total_prompts_tested: 0
benchmarks: {}
