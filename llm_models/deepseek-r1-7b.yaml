id: deepseek-r1:7b
name: DeepSeek R1 7B
type: llm_model
version: 1.0.0
provider:
  name: ollama
  api_type: ollama
adapter_config:
  type: ollama
  model: deepseek-r1:7b-qwen-distill-q4_K_M
  base_url: null
  base_url_env: null
parameters:
  temperature: 0.7
  max_tokens: 500
  top_p: 0.9
capabilities:
  structured_output: true
  reasoning: false
  tool_calling: false
  streaming: true
  vision: false
  function_calling: false
cost:
  per_1m_input_tokens: 0.0
  per_1m_output_tokens: 0.0
  currency: USD
  notes: Free local inference
performance:
  avg_latency_ms: 3302.8
  tokens_per_second: 0
  context_window: 16384

# Context variants for strategy-aware context management
context_variants:
  throughput: {context_window: 4096}   # Fast for reasoning model
  balanced: {context_window: 8192}     # Good balance
  quality: {context_window: 16384}     # Full context for complex reasoning
metadata:
  description: DeepSeek R1 7B with Qwen distillation - Reasoning-focused
  category: reasoning
  status: stable
  release_date: '2025-01-01'
  last_verified: '2025-11-05'
  last_benchmarked: '2025-11-12T14:06:51.898466'
  recommended_for: []
  not_recommended_for:
  - Real-time applications
requirements:
  gpu_vram_mb: 4500
  system_ram_mb: 4000
  cpu_cores: 2.0
hardware_detection:
  execution_mode: local_gpu
  optimal_concurrency: 8
  max_concurrency: 16
resource_class:
  class: B
  vram_gb: 4.5
  parallel_capacity_spark: 25
  recommended_parallel: 20
  throughput_tier: moderate
  tokens_per_second_single: 20
tier: B
tier_rank: 5
base_scores:
  source: DeepSeek R1 Technical Report, January 2025
  source_url: https://github.com/deepseek-ai/DeepSeek-R1
  captured_date: '2025-12-05'
  raw_benchmarks:
    mmlu: 65.8
    gsm8k: 78.5
    arc_challenge: 62.4
    hellaswag: 74.2
    humaneval: 38.2
    ifeval: 68.5
    math: 58.4
  notes: DeepSeek R1 7B - distilled reasoning model from Qwen
dimension_scores:
  summarization: 68
  reasoning: 82
  instruction: 70
  analysis: 72
  coding: 55
  speed: 65
  context_efficiency: 78
pom_benchmarks:
  last_calibration: '2025-12-05'
  calibration_version: '1.0'
  prompts_tested: {}
  aggregate:
    avg_quality: 70
    avg_latency_ms: 1100
    total_prompts_tested: 0
benchmarks:
  last_run: '2025-11-12T14:06:51.898460'
  tests_passed: 5/5
  summary:
    avg_latency_ms: 3302.8
    avg_tokens_per_second: 0.0
    total_time_seconds: 16.51
  tests:
    simple_instruction:
      latency_ms: 1784.9
      tokens_per_second: 0.0
      time_seconds: 1.785
      estimated_tokens: 0.0
    structured_output:
      latency_ms: 1357.6
      tokens_per_second: 0.0
      time_seconds: 1.358
      estimated_tokens: 0.0
    reasoning:
      latency_ms: 4830.2
      tokens_per_second: 0.0
      time_seconds: 4.83
      estimated_tokens: 0.0
    summarization:
      latency_ms: 2487.2
      tokens_per_second: 0.0
      time_seconds: 2.487
      estimated_tokens: 0.0
    code_generation:
      latency_ms: 6054.0
      tokens_per_second: 0.0
      time_seconds: 6.054
      estimated_tokens: 0.0
