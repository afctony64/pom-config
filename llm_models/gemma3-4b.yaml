id: "gemma3:4b"
name: Gemma 3 4B
type: llm_model
version: "1.0.0"
provider: {name: ollama, api_type: ollama}
adapter_config: {type: ollama, model: "gemma3:4b", base_url: null, base_url_env: OLLAMA_HOST}
parameters: {temperature: 0.7, max_tokens: 1000, top_p: 0.9}
capabilities: {structured_output: true, reasoning: false, tool_calling: false, streaming: true, vision: true, function_calling: false, multimodal: true}
cost: {per_1m_input_tokens: 0.0, per_1m_output_tokens: 0.0, currency: USD, notes: "Free local"}
performance: {avg_latency_ms: 350, tokens_per_second: 110, context_window: 8192}  # Practical (theoretical: 128K)
metadata: {description: "Google Gemma 3 4B - Fast multimodal", category: general, status: stable, release_date: "2025-03-01", last_verified: "2025-12-05", family: gemma3, family_tier: entry, recommended_for: ["Fast image analysis"], not_recommended_for: ["Complex reasoning"]}
requirements: {gpu_vram_mb: 3300, system_ram_mb: 4000, cpu_cores: 2.0}
hardware_detection: {execution_mode: local_gpu, optimal_concurrency: 24, max_concurrency: 38, dgx_spark: {optimal_parallel_workers: 32, recommended_batch_size: 4, memory_usage_gb: 3.3, tokens_per_second: 110}}
resource_class: {class: A, vram_gb: 3.3, parallel_capacity_spark: 38, recommended_parallel: 32, throughput_tier: high, tokens_per_second_single: 110}
tier: C
tier_rank: 3
dimension_scores: {summarization: 68, reasoning: 62, instruction: 70, analysis: 65, coding: 58, speed: 92, context_efficiency: 82}
pom_benchmarks: {last_calibration: "2025-12-05", calibration_version: "2.0", aggregate: {avg_quality: 65, avg_latency_ms: 350, total_prompts_tested: 0}}
benchmarks: {}
