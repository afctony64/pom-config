# ==============================================================================
# MODEL CARD TEMPLATE
# ==============================================================================
# Copy this file and rename to: {family}-{size}.yaml
# Example: gemma3-12b.yaml, qwen3-8b.yaml, llama3.2-3b.yaml
#
# See README.md for full onboarding guide
# ==============================================================================

id: model-id                      # Unique, lowercase, hyphenated (e.g., gemma3-12b)
name: Model Name                  # Human readable (e.g., Gemma 3 12B)
type: llm_model                   # Always llm_model
version: "1.0.0"                  # Semantic version of this card

# === PROVIDER CONFIGURATION ===
provider:
  name: ollama                    # ollama | openai | anthropic
  api_type: ollama                # Match provider name

# === ADAPTER CONFIGURATION ===
adapter_config:
  type: ollama                    # Type of adapter
  model: model:tag                # Exact model tag (e.g., gemma3:12b)
  base_url: null                  # Custom base URL (usually null)
  base_url_env: null              # Env var for base URL

# === DEFAULT PARAMETERS ===
parameters:
  temperature: 0.7                # Default: 0.7 for general, 0.1 for deterministic
  max_tokens: 1500                # Default max output tokens
  top_p: 0.9                      # Nucleus sampling parameter

# === CAPABILITIES ===
capabilities:
  structured_output: true         # Can produce JSON/structured output
  reasoning: false                # Good at complex reasoning (set true for 12B+)
  tool_calling: false             # Can call tools/functions
  streaming: true                 # Supports streaming responses
  vision: false                   # Can process images (multimodal)
  function_calling: false         # OpenAI-style function calling
  multimodal: false               # General multimodal flag

# === COST ===
cost:
  per_1m_input_tokens: 0.0        # Cost per 1M input tokens (0 for local)
  per_1m_output_tokens: 0.0       # Cost per 1M output tokens
  currency: USD
  notes: "Free local inference"   # Or pricing notes for cloud models

# === PERFORMANCE ===
performance:
  avg_latency_ms: 0               # To be populated by benchmarks
  tokens_per_second: 0            # To be populated by benchmarks
  # context_window is the DEFAULT local window (optimize throughput/memory on Spark/Mac).
  # Track official/max separately in comments or explicit fields if/when added.
  context_window: 128000          # From official model specs (often set lower locally)
  # Optional: document official/extended max context (kept separate from default local)
  # context_window_max: 131072
  
  # Backend-specific benchmarks (populated by benchmark tool)
  # backends:
  #   mac_metal:
  #     tokens_per_second: 90
  #     avg_latency_ms: 200
  #     optimal_parallelism: 6
  #     memory_bandwidth_gbs: 546
  #   spark_cuda:
  #     tokens_per_second: 45
  #     avg_latency_ms: 400
  #     optimal_parallelism: 4
  #     memory_bandwidth_gbs: 273
  #   groq:
  #     tokens_per_second: 500
  #     avg_latency_ms: 80
  #     model_id: "model-name"

# Optional: Named context variants (if you want the proxy to support context_variant selection)
# context_variants:
#   throughput: {context_window: 8192, notes: "Max throughput / low KV cache"}
#   balanced:   {context_window: 16384}
#   quality:    {context_window: 32768}
#   extended:   {context_window: 131072, notes: "May require YaRN/RoPE scaling; slower"}

# ==============================================================================
# PRIORITY CONFIGURATION (Performance Optimization)
# ==============================================================================
# Priority tiers control routing to maximize efficiency for different workloads:
#   - ultra: Groq LPU (~500 tok/s) - instant response, user-facing
#   - fast: Mac Metal (~90 tok/s) - interactive, low latency
#   - batch: Spark CUDA (~45 tok/s) - background processing
#   - max_throughput: ollama-lb (~135 tok/s) - parallel load balanced
#
# priority_config:
#   default: fast                   # Default priority when not specified
#   supports: [ultra, fast, batch, max_throughput]  # Which priorities this model supports
#   groq_equivalent: "model-name"   # Groq model ID for ultra priority
#   
#   # Task-specific priority overrides
#   # When a prompt matches these patterns, use the specified priority
#   task_priorities:
#     fact_extraction: batch        # Accuracy over speed
#     user_chat: fast               # Low latency for UX
#     summarization: batch          # Background job
#     code_generation: fast         # Interactive coding
#     analysis: batch               # Thorough processing
#     real_time: ultra              # Instant response needed
#
# routing: [mac, spark]             # Default routing order (mac primary)
# routing_priority:
#   ultra: groq
#   fast: mac
#   batch: spark
#   max_throughput: ollama-lb

# === METADATA ===
metadata:
  description: "Model description here"
  category: general               # general | coding | reasoning | creative
  status: stable                  # stable | beta | experimental | deprecated
  release_date: "2025-01-01"      # Model release date
  last_verified: "2025-12-05"     # Last time we verified this works
  family: model_family            # e.g., gemma3, qwen3, llama3
  family_tier: balanced           # entry | balanced | quality | premium
  recommended_for:
    - "Use case 1"
    - "Use case 2"
    - "Use case 3"
  not_recommended_for:
    - "Anti-pattern 1"
    - "Anti-pattern 2"

# === HARDWARE REQUIREMENTS ===
requirements:
  gpu_vram_mb: 0                  # Minimum VRAM in MB (Q4 quantized)
  system_ram_mb: 0                # Minimum system RAM in MB
  cpu_cores: 2.0                  # Minimum CPU cores

# === HARDWARE-SPECIFIC OPTIMIZATION ===
hardware_detection:
  execution_mode: local_gpu       # local_gpu | local_cpu | cloud
  optimal_concurrency: 8          # Default optimal parallel workers
  max_concurrency: 16             # Maximum safe parallel workers

  # DGX Spark / 128GB unified memory (Apple M-series Max/Ultra)
  dgx_spark:
    optimal_parallel_workers: 0   # Recommended parallel workers
    recommended_batch_size: 1     # Batch size per worker
    memory_usage_gb: 0.0          # Actual memory per instance
    estimated_throughput: 0.0     # Items/second estimate
    tokens_per_second: 0          # Tokens/second on this hardware
    notes: "Notes for Spark users"

  # Consumer GPUs 24GB+ (RTX 4090, A5000, A6000)
  gpu_24gb_plus:
    optimal_parallel_workers: 0
    recommended_batch_size: 1

  # Consumer GPUs 8-16GB (RTX 3080, RTX 4070)
  gpu_8gb_plus:
    optimal_parallel_workers: 0
    recommended_batch_size: 1

  # CPU-only inference
  cpu_only:
    optimal_parallel_workers: 1
    recommended_batch_size: 1
    notes: "CPU inference - slow but works"

# ==============================================================================
# VAST.AI SERVERLESS CONFIGURATION (Cloud GPU Fallback)
# ==============================================================================
# Enable cloud GPU inference via Vast.ai Serverless when local is unavailable.
# Set enabled: true and configure endpoint details to enable.
# See: PomSpark/docs/VASTAI_CLOUD_OLLAMA.md for setup guide.
#
# vastai_config:
#   enabled: true                            # Enable Vast.ai for this model
#   endpoint_name: "vLLM-Model-Name"         # Vast.ai endpoint name
#   huggingface_model: "org/model-name"      # HuggingFace model ID for vLLM
#   gpu_tier: "consumer"                     # consumer | professional | datacenter
#   skip_local: false                        # true = skip local Ollama, go direct to Vast.ai
#   cold_workers: 0                          # 0=free but 3-5min cold start, 1+=~$6.50/mo each
#   disk_space_gb: 100                       # Minimum host disk (GB) to avoid HF/vLLM download failures
#   gpu_requirements: "gpu_ram>=20 num_gpus=1 inet_down>100"
#   template_hash: null                      # Uses default vLLM template if null
#   estimated_cold_start_seconds: 180        # 180 if cold_workers=0, 30 if cold_workers>=1
#   priority: 0                              # Higher = preferred for cloud fallback

# === RESOURCE CLASS (for benchmarking system) ===
resource_class:
  class: B                        # A=Light(<6GB), B=Balanced(6-11GB), C=Heavy(12GB+)
  vram_gb: 0.0                    # Actual VRAM usage
  parallel_capacity_spark: 0      # Max parallel instances on 128GB
  recommended_parallel: 0         # Recommended parallel for batch
  throughput_tier: moderate       # high | moderate | low
  tokens_per_second_single: 0     # Single-instance tok/s

# ==============================================================================
# MODEL RANKING SYSTEM
# ==============================================================================
# Tier determines which prompts can use this model (based on complexity)
# S=flagship (complex+premium), A=quality (complex), B=balanced (medium), C=fast (simple)
tier: B                           # S | A | B | C
tier_rank: 1                      # Position within tier (1=best in tier)

# === DIMENSION SCORES (0-100) ===
# Scores based on public benchmarks + Pom testing
# Used by model selection algorithm to match prompts to best model
dimension_scores:
  summarization: 0                # MMLU-style text synthesis
  reasoning: 0                    # GSM8K/ARC logical reasoning
  instruction: 0                  # IFEval instruction following
  analysis: 0                     # Data interpretation/extraction
  coding: 0                       # HumanEval code generation
  speed: 0                        # Normalized tokens/sec (100=fastest)
  context_efficiency: 0           # Long context performance (NIAH)

# === POM-SPECIFIC BENCHMARKS ===
# Results from running actual Pomothy prompts
# Run: docker exec pomothy-backend python -m pom_core.tools.benchmark_model --model-card {id} --pom-prompts
pom_benchmarks:
  last_calibration: ""            # ISO date of last calibration
  calibration_version: "1.0"      # Calibration system version
  prompts_tested: {}              # Filled by benchmark tool
  # Example:
  # prompts_tested:
  #   research_synopsis:
  #     quality_score: 84          # Human eval 0-100
  #     latency_ms: 2800
  #     tokens_generated: 350
  #     cost_per_run: 0.0
  aggregate:
    avg_quality: 0                # Average quality across prompts
    avg_latency_ms: 0             # Average latency
    total_prompts_tested: 0       # Number of prompts tested

# === GENERIC BENCHMARKS (populated by benchmark tool) ===
# Run: docker exec pomothy-backend python -m pom_core.tools.benchmark_model --model-card {id}
benchmarks: {}

# ==============================================================================
# DEPRECATION (only add if model is deprecated)
# ==============================================================================
# deprecation:
#   deprecated: true
#   deprecated_date: "2025-12-05"
#   sunset_date: "2026-06-01"
#   reason: "Superseded by newer model family"
#   migration_guide: |
#     Recommended migrations:
#     - old_model â†’ new_model (reason)
#   replacement_model: new-model-id
#   replacement_model_alt: alt-model-id
