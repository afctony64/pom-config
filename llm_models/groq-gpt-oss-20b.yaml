id: "groq-gpt-oss-20b"
aliases:
  - "groq:gpt-oss-20b"
name: Groq GPT-OSS 20B
type: llm_model
version: "1.0.0"

provider:
  name: groq
  api_type: chat

adapter_config:
  type: groq
  model: openai/gpt-oss-20b
  base_url: https://api.groq.com/openai/v1
  base_url_env: GROQ_API_KEY

parameters:
  temperature: 0.7
  max_tokens: 4000
  top_p: 0.9

capabilities:
  structured_output: true
  reasoning: true
  tool_calling: true
  streaming: true
  vision: false
  function_calling: true
  multimodal: false

cost:
  per_1m_input_tokens: 0.075
  per_1m_output_tokens: 0.30
  currency: USD
  notes: "Groq LPU - fastest 20B model, 50% cache discount available"

performance:
  avg_latency_ms: 120
  tokens_per_second: 1000
  context_window: 131072

metadata:
  description: "GPT-OSS 20B on Groq - 1000 tok/s, best throughput"
  category: general
  status: stable
  release_date: "2025-01-01"
  last_verified: "2025-12-18"
  family: gpt-oss
  family_tier: standard
  recommended_for:
    - "High throughput"
    - "Balanced quality/speed"
    - "Cost efficiency"
  not_recommended_for:
    - "Maximum quality"

requirements:
  gpu_vram_mb: 0
  system_ram_mb: 0
  cpu_cores: 0

hardware_detection:
  execution_mode: cloud

routing: [groq]

resource_class:
  class: A
  vram_gb: 0
  throughput_tier: high
  tokens_per_second_single: 1000

tier: A
tier_rank: 2

dimension_scores:
  summarization: 82
  reasoning: 80
  instruction: 84
  analysis: 80
  coding: 78
  speed: 99
  context_efficiency: 92

pom_benchmarks:
  last_calibration: "2025-12-18"
  calibration_version: "2.0"
  aggregate:
    avg_quality: 81
    avg_latency_ms: 120
    total_prompts_tested: 0

benchmarks: {}
