# =============================================================================
# üéöÔ∏è WORKLOAD PROFILES CONFIGURATION
# =============================================================================
#
# Defines resource allocations for different workload types.
# Switch profiles with: ./scripts/workload.sh <profile>
#
# Philosophy:
#   - Limited hardware (Mac 128GB + Spark 119GB/GB10 GPU) requires smart sharing
#   - GPU on Spark is shared between Ollama and 5 transformer instances
#   - Conscious profile switching before big jobs prevents overload
#   - Each profile optimizes for a specific workload type
#
# PRIMARY PROFILES (use these for PomAI workflows):
#   - ai             ‚Üí Researcher prompts, LLM extraction (heavy Ollama)
#   - data_pipeline  ‚Üí Domain/Page/Facts processing (heavy embeddings, no AI)
#   - balanced       ‚Üí Mixed workloads (default)
#
# UTILITY PROFILES:
#   - query_only     ‚Üí Read-only exploration (minimal resources)
#   - travel         ‚Üí Mac-only mode (Spark offline)
#
# =============================================================================

# -----------------------------------------------------------------------------
# HARDWARE REFERENCE
# -----------------------------------------------------------------------------
hardware:
  mac:
    total_ram: 128GB
    docker_allocation: 96GB
    system_reserve: 32GB
    gpu: "Apple Silicon Metal (unified memory)"
    cpu: "Apple Silicon (high core count)"
    notes:
      - "Metal acceleration works for Ollama"
      - "Transformers in Docker are CPU-only (no Metal support)"
      - "mac-transformers-metal runs NATIVE on host for Metal acceleration"
      
  spark:
    total_ram: 128GB  # Updated: 128GB total RAM
    gpu: "NVIDIA GB10 (dedicated VRAM)"
    cpu: "ARM64 (20+ cores)"
    notes:
      - "GPU shared between Ollama and transformers"
      - "5 transformer instances share single GPU"
      - "Running all services at max causes GPU memory contention"
      - "Browser pool: 25 contexts, priority service (20GB memory, 8 CPU cores)"

# -----------------------------------------------------------------------------
# WORKLOAD PROFILES
# -----------------------------------------------------------------------------
profiles:

  # ===========================================================================
  # AI - Priority LLM capacity for researcher prompts and fact extraction
  # ===========================================================================
  # Use this when running:
  #   - Researcher fact extraction (pomflow_cli, research_pipeline_cli Stage 4)
  #   - Heavy chat/completion workloads
  #   - Large model inference (70B+ models)
  #
  # Design: PRIORITY on Ollama, but other services still FUNCTIONAL for dev
  # ===========================================================================
  ai:
    description: "Priority LLM capacity for researcher prompts and AI extraction"
    use_case: "Researcher prompts, fact extraction, bulk LLM processing"
    warning: "‚ö†Ô∏è Embeddings reduced to 3 instances - good balance for LLM + queries"
    
    spark:
      ollama:
        enabled: true
        memory_limit: 80G        # Priority allocation for LLM
        memory_reserved: 64G
        num_parallel: 24         # High parallelism for batch processing
        max_loaded_models: 3     # Keep multiple models hot
        keep_alive: 2h           # Keep models loaded
      transformers:
        instances: 3             # Balanced - embeddings don't take much GPU memory
        enabled: [1, 2, 3]
        note: "3 instances provide good embedding throughput without impacting Ollama"
      weaviate:
        memory_limit: 72G        # MINIMUM 72GB - prevents memory-induced CPU spin loops
        memory_reserved: 48G
      backend:
        memory_limit: 24G        # FUNCTIONAL - can still run small jobs
        memory_reserved: 12G
        playwright_max_parallel: 20  # Maximum browser parallelism
        http_connection_limit: 750   # Reduced - AI profile focuses on LLM
        http_limit_per_host: 10      # Max per-domain connections
        http_keepalive_timeout: 5    # Standard cleanup interval
        crawl_max_concurrent: 50     # Reduced - AI profile focuses on LLM
      browser_pool:
        enabled: true
        pool_size: 10            # Reduced but functional for dev
        memory_limit: 8G
        cpus: 4
        note: "Reduced but functional for small dev jobs"
        
    mac:
      weaviate:
        memory_limit: 48G        # No GPU contention - native services run separately
        memory_reserved: 24G
        gomemlimit: 46GiB
        gogc: 50
      transformers_metal:
        enabled: true
        note: "Mac Metal transformers handle any embedding needs"
      ollama_metal:
        enabled: false
        note: "Use Spark Ollama - it has priority capacity"
      redis:
        memory_limit: 4G
        maxmemory: 2G
      pom_llm_proxy:
        memory_limit: 8G
        memory_reserved: 2G
      browser_pool:
        enabled: true
        pool_size: 3
        note: "Reduced but functional"

  # ===========================================================================
  # DATA_PIPELINE - Priority backend/embedding for Domain‚ÜíPage‚ÜíFacts workflow
  # ===========================================================================
  # Use this when running:
  #   - research_pipeline_cli (Stages 1-3)
  #   - page_facts_cli (bulk page processing)
  #   - domain_snapshot_cli (domain discovery)
  #   - Any bulk scraping or embedding work
  #
  # Design: Browser pool is SEPARATE from backend - backend doesn't need browser memory
  #         Backend only needs memory for Python workers (16GB is 2x base allocation)
  #         Weaviate needs more headroom (80GB) - current usage ~49.5GB (88%)
  #         Browser pool scales independently (50 contexts = 2x capacity)
  # ===========================================================================
  data_pipeline:
    description: "Priority backend/embedding for Domain/Page/Facts pipeline"
    use_case: "Domain snapshots, Page_intelligence, Page_facts, bulk embeddings"
    warning: "‚ö†Ô∏è LLM minimal (16GB) - 7B models only, use 'ai' for larger"
    
    spark:
      ollama:
        enabled: true            # FUNCTIONAL - minimal but can run 7B models
        memory_limit: 16G        # Enough for 7B models
        memory_reserved: 8G
        num_parallel: 2          # Minimal parallelism
        max_loaded_models: 1     # One model at a time
        keep_alive: 5m           # Quick unload to save memory
        note: "Minimal but functional - can run 7B models for dev"
      transformers:
        instances: 5             # ALL instances for maximum throughput
        enabled: [1, 2, 3, 4, 5]
        note: "All 5 instances for ~2,346 emb/sec throughput"
      weaviate:
        memory_limit: 80G        # Increased from 64G - current usage ~49.5GB (88%), need more headroom
        memory_reserved: 56G     # Increased from 40G
        async_indexing: true     # Enable async for faster ingestion
      backend:
        memory_limit: 16G        # Reduced from 56G - browser pool is separate service!
        memory_reserved: 8G      # Reduced from 40G - still 2x base allocation
        playwright_max_parallel: 20  # Maximum browser parallelism
        pids: 4096               # High PID limit for concurrent workers
        http_connection_limit: 2500  # 250 domains √ó 10 connections (Jan 2026)
        http_limit_per_host: 10      # Max per-domain connections
        http_keepalive_timeout: 5    # Standard cleanup interval
        crawl_max_concurrent: 200    # 200 domains concurrently
        note: "Browser pool is separate - backend only needs memory for workers, not browsers"
      browser_pool:
        enabled: true
        pool_size: 800           # 800 contexts for high throughput
        memory_limit: 56G        # 56GB balances with Weaviate (72GB) on 119GB system
        cpus: 18                 # High CPU for parallel rendering
        note: "Spark browser pool: 800 contexts for high throughput"
        
    mac:
      weaviate:
        memory_limit: 48G        # Increased: No GPU contention - Ollama/Transformers run natively
        memory_reserved: 24G
        gomemlimit: 46GiB        # 96% of limit for GC headroom
        gogc: 50                 # Aggressive GC for batch operations
        note: "Mac Weaviate has full priority - native services don't compete"
      transformers_metal:
        enabled: true
        note: "CRITICAL - Native Metal transformers for Mac embeddings"
      ollama_metal:
        enabled: false
        note: "Use Spark Ollama (minimal but functional)"
      redis:
        memory_limit: 4G
        maxmemory: 2G
      pom_llm_proxy:
        memory_limit: 4G
        memory_reserved: 1G
      browser_pool:
        enabled: true
        pool_size: 5
        memory_limit: 8G
        note: "Mac browser pool for Playwright domains"

  # ===========================================================================
  # BALANCED - Default mixed workload profile
  # ===========================================================================
  # Use this for:
  #   - General development
  #   - Light usage of all features
  #   - When unsure which profile to use
  # ===========================================================================
  balanced:
    description: "Default profile for mixed workloads"
    use_case: "General development, light usage of all features"
    warning: null
    
    spark:
      ollama:
        enabled: true
        memory_limit: 48G
        memory_reserved: 32G
        num_parallel: 12
        max_loaded_models: 2
        keep_alive: 30m
      transformers:
        instances: 3             # Balanced between LLM and embeddings
        enabled: [1, 2, 3]
      weaviate:
        memory_limit: 72G        # MINIMUM 72GB - prevents memory-induced CPU spin loops
        memory_reserved: 48G
      backend:
        memory_limit: 32G
        memory_reserved: 16G
        playwright_max_parallel: 20
        http_connection_limit: 1000  # 100 domains √ó 5 connections + headroom
        http_limit_per_host: 10      # Max per-domain connections
        http_keepalive_timeout: 5    # Standard cleanup interval
        crawl_max_concurrent: 100    # 100 domains concurrently (balanced)
      browser_pool:
        enabled: true
        pool_size: 25            # Full capacity available (priority service)
        memory_limit: 20G
        cpus: 8
        note: "Spark browser pool: 25 contexts available for balanced workloads"
        
    mac:
      weaviate:
        memory_limit: 48G        # No GPU contention - native services run separately
        memory_reserved: 24G
        gomemlimit: 46GiB
        gogc: 50
      transformers_metal:
        enabled: true
        note: "Native host service - start with ./scripts/mac-transformers-metal.sh"
      ollama_metal:
        enabled: false
        note: "Not needed - uses Spark Ollama"
      redis:
        memory_limit: 4G
        maxmemory: 2G
      pom_llm_proxy:
        memory_limit: 8G
        memory_reserved: 2G
      browser_pool:
        enabled: true
        pool_size: 3
        memory_limit: 4G

  # ===========================================================================
  # WEB_SCRAPING - Maximum browser pool for high-throughput scraping
  # ===========================================================================
  # Use this when running:
  #   - High-throughput web scraping (page_facts/page_intelligence)
  #   - Large batch ingestion into Weaviate
  #
  # Design: Browser pool is the priority service; backend is coordination-only.
  # ===========================================================================
  web_scraping:
    description: "Maximum browser pool for high-throughput scraping"
    use_case: "Bulk scraping and batch Weaviate updates"
    warning: "‚ö†Ô∏è LLM minimal (12GB) - 7B models only, optimized for scraping"
    
    spark:
      ollama:
        enabled: true
        memory_limit: 12G
        memory_reserved: 6G
        num_parallel: 2
        max_loaded_models: 1
        keep_alive: 5m
      transformers:
        instances: 5
        enabled: [1, 2, 3, 4, 5]
      weaviate:
        memory_limit: 72G
        memory_reserved: 48G
        async_indexing: true
      backend:
        memory_limit: 12G
        memory_reserved: 6G
        playwright_max_parallel: 100
        http_connection_limit: 3000
        http_limit_per_host: 15
        http_keepalive_timeout: 5
        crawl_max_concurrent: 300
      browser_pool:
        enabled: true
        pool_size: 800           # 800 contexts for high throughput
        memory_limit: 56G        # 56GB balances with Weaviate (72GB) on 119GB system
        cpus: 18
        note: "Maximum capacity browser pool for high-throughput scraping"
        
    mac:
      weaviate:
        memory_limit: 48G
        memory_reserved: 24G
        gomemlimit: 46GiB
        gogc: 50
      transformers_metal:
        enabled: true
      ollama_metal:
        enabled: false
      redis:
        memory_limit: 4G
        maxmemory: 2G
      pom_llm_proxy:
        memory_limit: 8G
        memory_reserved: 2G
      browser_pool:
        enabled: true
        pool_size: 3
        memory_limit: 4G

  # ===========================================================================
  # QUERY_ONLY - Minimal resources for read-only workloads
  # ===========================================================================
  query_only:
    description: "Minimal resources for search and query workloads"
    use_case: "Read-only exploration, vector search, low resource usage"
    warning: "‚ö†Ô∏è No LLM processing - Ollama stopped on Spark"
    
    spark:
      ollama:
        enabled: false           # Stopped to free GPU memory
      transformers:
        instances: 1             # Minimal for Weaviate queries
        enabled: [1]
      weaviate:
        memory_limit: 72G        # MINIMUM 72GB - prevents memory-induced CPU spin loops
        memory_reserved: 48G
      backend:
        enabled: false           # Not needed for queries
      browser_pool:
        enabled: false
        note: "Not needed for query-only workloads"
        
    mac:
      weaviate:
        memory_limit: 48G        # No GPU contention - native services run separately
        memory_reserved: 24G
        gomemlimit: 46GiB
        gogc: 50
      transformers_metal:
        enabled: true
      ollama_metal:
        enabled: true            # Use Mac for any LLM needs
        note: "Start with ./scripts/mac-ollama-metal.sh if LLM needed"
      redis:
        memory_limit: 2G
        maxmemory: 1G
      pom_llm_proxy:
        memory_limit: 4G
        memory_reserved: 1G
      browser_pool:
        enabled: false

  # ===========================================================================
  # NETWORK_FRIENDLY - Low connection count for shared network
  # ===========================================================================
  # Use this when:
  #   - Scraping interferes with home network (video calls, streaming)
  #   - Panel shows low bandwidth but network is unstable
  #   - Running 1000+ URL scrapes during peak hours
  #
  # Design: REDUCES CONNECTION COUNT, not just bandwidth
  #         Connection churn (not throughput) overwhelms NAT tables
  #         Cox Gateway, dumb switches struggle with 1000s of connections
  # ===========================================================================
  network_friendly:
    description: "Low connection count for shared network - reduces NAT table pressure"
    use_case: "Scraping during peak hours, video calls active, shared home network"
    warning: "‚ö†Ô∏è Scraping 5x slower but won't interfere with other network traffic"
    
    spark:
      ollama:
        enabled: true
        memory_limit: 48G
        memory_reserved: 32G
        num_parallel: 8            # Reduced parallelism
        max_loaded_models: 2
        keep_alive: 30m
      transformers:
        instances: 3
        enabled: [1, 2, 3]
      weaviate:
        memory_limit: 72G        # MINIMUM 72GB - prevents memory-induced CPU spin loops
        memory_reserved: 48G
      backend:
        memory_limit: 32G
        memory_reserved: 16G
        playwright_max_parallel: 5   # ‚ö†Ô∏è KEY: Reduced from 20 to 5
        request_delay_ms: 500        # Add delay between requests
        scrape_delay_ms: 500         # Delay between requests (gateway protection)
        max_concurrent_pages: 3      # Limit concurrent page processing
        http_connection_limit: 5000  # Testing: 5K to find network capacity
        http_limit_per_host: 3       # Reduced per-host connections
        http_keepalive_timeout: 3    # Faster cleanup to free NAT entries
        crawl_max_concurrent: 30     # Reduced from 200 to 30 domains
        note: "Reduced parallelism to prevent connection exhaustion"
      browser_pool:
        enabled: true
        pool_size: 3                 # ‚ö†Ô∏è Reduced from 25 to 3 contexts
        memory_limit: 4G             # Reduced memory for fewer contexts
        cpus: 2                      # Reduced CPU allocation
        note: "3 contexts = ~3 concurrent page loads, prevents NAT exhaustion"
        
    mac:
      weaviate:
        memory_limit: 48G        # No GPU contention - native services run separately
        memory_reserved: 24G
        gomemlimit: 46GiB
        gogc: 50
      transformers_metal:
        enabled: true
      ollama_metal:
        enabled: false
        note: "Use Spark Ollama"
      redis:
        memory_limit: 4G
        maxmemory: 2G
      pom_llm_proxy:
        memory_limit: 8G
        memory_reserved: 2G
      browser_pool:
        enabled: true
        pool_size: 2                 # Minimal for network-friendly mode
        memory_limit: 2G

  # ===========================================================================
  # TRAVEL - Mac-only mode (Spark offline)
  # ===========================================================================
  travel:
    description: "Mac-only mode when Spark is unavailable"
    use_case: "Working offline, Spark unreachable, travel"
    warning: "‚ö†Ô∏è No GPU LLM - using Mac Metal (slower for large models)"
    
    spark:
      # All Spark services disabled
      ollama:
        enabled: false
      transformers:
        enabled: false
      weaviate:
        enabled: false
      backend:
        enabled: false
      browser_pool:
        enabled: false
        note: "Spark offline - browser pool unavailable"
        
    mac:
      weaviate:
        memory_limit: 48G        # No GPU contention - native services run separately
        memory_reserved: 24G
        gomemlimit: 46GiB
        gogc: 50
        note: "Full 48GB available - native services don't compete"
      transformers_metal:
        enabled: true
        note: "CRITICAL - Only embedding service available"
      ollama_metal:
        enabled: true
        memory_limit: 48G
        memory_reserved: 32G
        num_parallel: 4
        max_loaded_models: 2
      redis:
        memory_limit: 4G
        maxmemory: 2G
      pom_llm_proxy:
        memory_limit: 8G
        memory_reserved: 2G
      browser_pool:
        enabled: false

# -----------------------------------------------------------------------------
# QUICK REFERENCE - Resource totals by profile
# -----------------------------------------------------------------------------
#
# Design: Profiles PRIORITIZE resources, not disable services.
#         Small dev jobs still work even when a profile is focused.
#
# SPARK RESOURCES (Weaviate minimum 72GB - prevents memory spin loops):
# Profile          | Ollama | Weaviate | Backend | Transformers | Browser Pool | HTTP Conn | Best For
# -----------------|--------|----------|---------|--------------|--------------|-----------|------------------
# ai               | 80 GB ‚òÖ| 72 GB    | 24 GB   | 3 instances  | 10 contexts  | 750       | Researcher prompts
# data_pipeline    | 16 GB  | 80 GB ‚òÖ  | 16 GB   | 5 instances  | 800 contexts‚òÖ| 1500      | Domain/Page/Facts
# balanced         | 48 GB  | 72 GB    | 32 GB   | 3 instances  | 25 contexts  | 1000      | General dev
# web_scraping     | 12 GB  | 72 GB    | 12 GB   | 5 instances  | 800 contexts‚òÖ| 3000      | Bulk scraping
# network_friendly | 48 GB  | 72 GB    | 32 GB   | 3 instances  | 5 contexts   | 500       | Shared network
# query_only       | OFF    | 72 GB    | OFF     | 1 instance   | Disabled     | N/A       | Search/exploration
# travel           | N/A    | N/A      | N/A     | N/A          | Disabled     | N/A       | Offline work
#
# MAC RESOURCES (ALL PROFILES):
# Service          | Limit  | Reserved | GOMEMLIMIT | GOGC | Notes
# -----------------|--------|----------|------------|------|---------------------------
# mac-weaviate     | 48 GB  | 24 GB    | 46 GiB     | 50   | No GPU contention - native services
# mac-redis        | 4 GB   | 2 GB     | N/A        | N/A  | Cache
# pom-llm-proxy    | 8 GB   | 2 GB     | N/A        | N/A  | LLM routing
#
# ‚òÖ = Priority allocation for this profile
#
# MAC WEAVIATE RATIONALE:
#   - Mac has 128GB RAM, Docker allocated 96GB
#   - Ollama runs NATIVELY (mac-ollama-metal) - not in Docker
#   - Transformers run NATIVELY (mac-transformers-metal) - not in Docker
#   - 48GB for Weaviate provides ample headroom for HNSW indexes and batch operations
#   - GOGC=50 for aggressive garbage collection during batch writes
#
# data_pipeline rationale:
#   - Browser pool is SEPARATE from backend - backend doesn't need browser memory
#   - Backend 16GB is 2x base allocation (8GB) - plenty for Python workers
#   - Weaviate 80GB addresses 88% utilization (current usage ~49.5GB)
#   - Browser pool 50 contexts = 2x capacity increase (separate service)
#
# Browser Pool (Spark):
#   - Pool Size: 10-400 contexts (profile dependent)
#   - Memory: 8-56GB limit (data_pipeline/web_scraping: 56GB for 400 contexts)
#   - CPU: 4-18 cores (data_pipeline/web_scraping: 18 cores for 400 contexts)
#   - Available in: ai (reduced), data_pipeline, balanced profiles
#   - SEPARATE SERVICE - not part of backend allocation!
#
# WORKFLOW GUIDE:
#   1. ./scripts/workload.sh data_pipeline  # Process domains, pages, facts
#   2. ./scripts/workload.sh ai             # Run researchers
#   3. ./scripts/workload.sh balanced       # Return to default
#
# -----------------------------------------------------------------------------
