# =============================================================================
# üéöÔ∏è WORKLOAD PROFILES CONFIGURATION
# =============================================================================
#
# Defines resource allocations for different workload types.
# Switch profiles with: ./scripts/workload.sh <profile>
#
# Philosophy:
#   - Limited hardware (Mac 128GB + Spark 119GB/GB10 GPU) requires smart sharing
#   - GPU on Spark is shared between Ollama and 5 transformer instances
#   - Conscious profile switching before big jobs prevents overload
#   - Each profile optimizes for a specific workload type
#
# PRIMARY PROFILES (use these for PomAI workflows):
#   - ai             ‚Üí Researcher prompts, LLM extraction (heavy Ollama)
#   - data_pipeline  ‚Üí Domain/Page/Facts processing (heavy embeddings, no AI)
#   - balanced       ‚Üí Mixed workloads (default)
#
# UTILITY PROFILES:
#   - query_only     ‚Üí Read-only exploration (minimal resources)
#   - travel         ‚Üí Mac-only mode (Spark offline)
#
# =============================================================================

# -----------------------------------------------------------------------------
# HARDWARE REFERENCE
# -----------------------------------------------------------------------------
hardware:
  mac:
    total_ram: 128GB
    docker_allocation: 96GB
    system_reserve: 32GB
    gpu: "Apple Silicon Metal (unified memory)"
    cpu: "Apple Silicon (high core count)"
    notes:
      - "Metal acceleration works for Ollama"
      - "Transformers in Docker are CPU-only (no Metal support)"
      - "mac-transformers-metal runs NATIVE on host for Metal acceleration"
      
  spark:
    total_ram: 128GB  # Updated: 128GB total RAM
    gpu: "NVIDIA GB10 (dedicated VRAM)"
    cpu: "ARM64 (20+ cores)"
    notes:
      - "GPU shared between Ollama and transformers"
      - "5 transformer instances share single GPU"
      - "Running all services at max causes GPU memory contention"
      - "Browser pool: 25 contexts, priority service (20GB memory, 8 CPU cores)"

# -----------------------------------------------------------------------------
# WORKLOAD PROFILES
# -----------------------------------------------------------------------------
profiles:

  # ===========================================================================
  # AI - Priority LLM capacity for researcher prompts and fact extraction
  # ===========================================================================
  # Use this when running:
  #   - Researcher fact extraction (pomflow_cli, research_pipeline_cli Stage 4)
  #   - Heavy chat/completion workloads
  #   - Large model inference (70B+ models)
  #
  # Design: PRIORITY on Ollama, but other services still FUNCTIONAL for dev
  # ===========================================================================
  ai:
    description: "Priority LLM capacity for researcher prompts and AI extraction"
    use_case: "Researcher prompts, fact extraction, bulk LLM processing"
    warning: "‚ö†Ô∏è Embeddings reduced to 3 instances - good balance for LLM + queries"
    
    spark:
      ollama:
        enabled: true
        memory_limit: 80G        # Priority allocation for LLM
        memory_reserved: 64G
        num_parallel: 24         # High parallelism for batch processing
        max_loaded_models: 3     # Keep multiple models hot
        keep_alive: 2h           # Keep models loaded
      transformers:
        instances: 3             # Balanced - embeddings don't take much GPU memory
        enabled: [1, 2, 3]
        note: "3 instances provide good embedding throughput without impacting Ollama"
      weaviate:
        memory_limit: 32G        # FUNCTIONAL - can still do queries + small writes
        memory_reserved: 20G
      backend:
        memory_limit: 24G        # FUNCTIONAL - can still run small jobs
        memory_reserved: 12G
        playwright_max_parallel: 20  # Maximum browser parallelism
      browser_pool:
        enabled: true
        pool_size: 10            # Reduced but functional for dev
        memory_limit: 8G
        note: "Reduced but functional for small dev jobs"
        
    mac:
      weaviate:
        memory_limit: 16G
        memory_reserved: 8G
      transformers_metal:
        enabled: true
        note: "Mac Metal transformers handle any embedding needs"
      ollama_metal:
        enabled: false
        note: "Use Spark Ollama - it has priority capacity"
      redis:
        memory_limit: 4G
        maxmemory: 2G
      pom_llm_proxy:
        memory_limit: 8G
        memory_reserved: 2G
      browser_pool:
        enabled: true
        pool_size: 3
        note: "Reduced but functional"

  # ===========================================================================
  # DATA_PIPELINE - Priority backend/embedding for Domain‚ÜíPage‚ÜíFacts workflow
  # ===========================================================================
  # Use this when running:
  #   - research_pipeline_cli (Stages 1-3)
  #   - page_facts_cli (bulk page processing)
  #   - domain_snapshot_cli (domain discovery)
  #   - Any bulk scraping or embedding work
  #
  # Design: Browser pool is SEPARATE from backend - backend doesn't need browser memory
  #         Backend only needs memory for Python workers (16GB is 2x base allocation)
  #         Weaviate needs more headroom (80GB) - current usage ~49.5GB (88%)
  #         Browser pool scales independently (50 contexts = 2x capacity)
  # ===========================================================================
  data_pipeline:
    description: "Priority backend/embedding for Domain/Page/Facts pipeline"
    use_case: "Domain snapshots, Page_intelligence, Page_facts, bulk embeddings"
    warning: "‚ö†Ô∏è LLM minimal (16GB) - 7B models only, use 'ai' for larger"
    
    spark:
      ollama:
        enabled: true            # FUNCTIONAL - minimal but can run 7B models
        memory_limit: 16G        # Enough for 7B models
        memory_reserved: 8G
        num_parallel: 2          # Minimal parallelism
        max_loaded_models: 1     # One model at a time
        keep_alive: 5m           # Quick unload to save memory
        note: "Minimal but functional - can run 7B models for dev"
      transformers:
        instances: 5             # ALL instances for maximum throughput
        enabled: [1, 2, 3, 4, 5]
        note: "All 5 instances for ~2,346 emb/sec throughput"
      weaviate:
        memory_limit: 80G        # Increased from 64G - current usage ~49.5GB (88%), need more headroom
        memory_reserved: 56G     # Increased from 40G
        async_indexing: true     # Enable async for faster ingestion
      backend:
        memory_limit: 16G        # Reduced from 56G - browser pool is separate service!
        memory_reserved: 8G      # Reduced from 40G - still 2x base allocation
        playwright_max_parallel: 20  # Maximum browser parallelism
        pids: 4096               # High PID limit for concurrent workers
        note: "Browser pool is separate - backend only needs memory for workers, not browsers"
      browser_pool:
        enabled: true
        pool_size: 50            # Increased from 25 - 2x capacity (50 contexts)
        memory_limit: 48G         # Increased from 24G - supports 50 contexts comfortably
        cpus: 14                  # Increased from 8 - more CPU for parallel rendering
        note: "Spark browser pool: 50 parallel contexts, priority service (browser pool is separate from backend)"
        
    mac:
      weaviate:
        memory_limit: 24G
        memory_reserved: 16G
      transformers_metal:
        enabled: true
        note: "CRITICAL - Native Metal transformers for Mac embeddings"
      ollama_metal:
        enabled: false
        note: "Use Spark Ollama (minimal but functional)"
      redis:
        memory_limit: 4G
        maxmemory: 2G
      pom_llm_proxy:
        memory_limit: 4G
        memory_reserved: 1G
      browser_pool:
        enabled: true
        pool_size: 5
        memory_limit: 8G
        note: "Mac browser pool for Playwright domains"

  # ===========================================================================
  # BALANCED - Default mixed workload profile
  # ===========================================================================
  # Use this for:
  #   - General development
  #   - Light usage of all features
  #   - When unsure which profile to use
  # ===========================================================================
  balanced:
    description: "Default profile for mixed workloads"
    use_case: "General development, light usage of all features"
    warning: null
    
    spark:
      ollama:
        enabled: true
        memory_limit: 48G
        memory_reserved: 32G
        num_parallel: 12
        max_loaded_models: 2
        keep_alive: 30m
      transformers:
        instances: 3             # Balanced between LLM and embeddings
        enabled: [1, 2, 3]
      weaviate:
        memory_limit: 40G
        memory_reserved: 24G
      backend:
        memory_limit: 32G
        memory_reserved: 16G
        playwright_max_parallel: 20
      browser_pool:
        enabled: true
        pool_size: 25            # Full capacity available (priority service)
        memory_limit: 20G
        cpus: 8
        note: "Spark browser pool: 25 contexts available for balanced workloads"
        
    mac:
      weaviate:
        memory_limit: 16G
        memory_reserved: 8G
      transformers_metal:
        enabled: true
        note: "Native host service - start with ./scripts/mac-transformers-metal.sh"
      ollama_metal:
        enabled: false
        note: "Not needed - uses Spark Ollama"
      redis:
        memory_limit: 4G
        maxmemory: 2G
      pom_llm_proxy:
        memory_limit: 8G
        memory_reserved: 2G
      browser_pool:
        enabled: true
        pool_size: 3
        memory_limit: 4G

  # ===========================================================================
  # QUERY_ONLY - Minimal resources for read-only workloads
  # ===========================================================================
  query_only:
    description: "Minimal resources for search and query workloads"
    use_case: "Read-only exploration, vector search, low resource usage"
    warning: "‚ö†Ô∏è No LLM processing - Ollama stopped on Spark"
    
    spark:
      ollama:
        enabled: false           # Stopped to free GPU memory
      transformers:
        instances: 1             # Minimal for Weaviate queries
        enabled: [1]
      weaviate:
        memory_limit: 48G        # Maximized for search performance
        memory_reserved: 32G
      backend:
        enabled: false           # Not needed for queries
      browser_pool:
        enabled: false
        note: "Not needed for query-only workloads"
        
    mac:
      weaviate:
        memory_limit: 24G        # Maximized for search
        memory_reserved: 16G
      transformers_metal:
        enabled: true
      ollama_metal:
        enabled: true            # Use Mac for any LLM needs
        note: "Start with ./scripts/mac-ollama-metal.sh if LLM needed"
      redis:
        memory_limit: 2G
        maxmemory: 1G
      pom_llm_proxy:
        memory_limit: 4G
        memory_reserved: 1G
      browser_pool:
        enabled: false

  # ===========================================================================
  # NETWORK_FRIENDLY - Low connection count for shared network
  # ===========================================================================
  # Use this when:
  #   - Scraping interferes with home network (video calls, streaming)
  #   - Panel shows low bandwidth but network is unstable
  #   - Running 1000+ URL scrapes during peak hours
  #
  # Design: REDUCES CONNECTION COUNT, not just bandwidth
  #         Connection churn (not throughput) overwhelms NAT tables
  #         Cox Gateway, dumb switches struggle with 1000s of connections
  # ===========================================================================
  network_friendly:
    description: "Low connection count for shared network - reduces NAT table pressure"
    use_case: "Scraping during peak hours, video calls active, shared home network"
    warning: "‚ö†Ô∏è Scraping 5x slower but won't interfere with other network traffic"
    
    spark:
      ollama:
        enabled: true
        memory_limit: 48G
        memory_reserved: 32G
        num_parallel: 8            # Reduced parallelism
        max_loaded_models: 2
        keep_alive: 30m
      transformers:
        instances: 3
        enabled: [1, 2, 3]
      weaviate:
        memory_limit: 40G
        memory_reserved: 24G
      backend:
        memory_limit: 32G
        memory_reserved: 16G
        playwright_max_parallel: 5   # ‚ö†Ô∏è KEY: Reduced from 20 to 5
        request_delay_ms: 500        # Add delay between requests
        note: "Reduced parallelism to prevent connection exhaustion"
      browser_pool:
        enabled: true
        pool_size: 5                 # ‚ö†Ô∏è KEY: Reduced from 25 to 5 contexts
        memory_limit: 4G             # Reduced memory for fewer contexts
        cpus: 2                      # Reduced CPU allocation
        note: "5 contexts = ~5 concurrent page loads, prevents NAT exhaustion"
        
    mac:
      weaviate:
        memory_limit: 16G
        memory_reserved: 8G
      transformers_metal:
        enabled: true
      ollama_metal:
        enabled: false
        note: "Use Spark Ollama"
      redis:
        memory_limit: 4G
        maxmemory: 2G
      pom_llm_proxy:
        memory_limit: 8G
        memory_reserved: 2G
      browser_pool:
        enabled: true
        pool_size: 2                 # Minimal for network-friendly mode
        memory_limit: 2G

  # ===========================================================================
  # TRAVEL - Mac-only mode (Spark offline)
  # ===========================================================================
  travel:
    description: "Mac-only mode when Spark is unavailable"
    use_case: "Working offline, Spark unreachable, travel"
    warning: "‚ö†Ô∏è No GPU LLM - using Mac Metal (slower for large models)"
    
    spark:
      # All Spark services disabled
      ollama:
        enabled: false
      transformers:
        enabled: false
      weaviate:
        enabled: false
      backend:
        enabled: false
      browser_pool:
        enabled: false
        note: "Spark offline - browser pool unavailable"
        
    mac:
      weaviate:
        memory_limit: 32G        # Maximized since Spark is offline
        memory_reserved: 20G
      transformers_metal:
        enabled: true
        note: "CRITICAL - Only embedding service available"
      ollama_metal:
        enabled: true
        memory_limit: 48G
        memory_reserved: 32G
        num_parallel: 4
        max_loaded_models: 2
      redis:
        memory_limit: 4G
        maxmemory: 2G
      pom_llm_proxy:
        memory_limit: 8G
        memory_reserved: 2G
      browser_pool:
        enabled: false

# -----------------------------------------------------------------------------
# QUICK REFERENCE - Resource totals by profile
# -----------------------------------------------------------------------------
#
# Design: Profiles PRIORITIZE resources, not disable services.
#         Small dev jobs still work even when a profile is focused.
#
# Profile          | Ollama | Weaviate | Backend | Transformers | Browser Pool | HTTP Conn | Best For
# -----------------|--------|----------|---------|--------------|--------------|-----------|------------------
# ai               | 80 GB ‚òÖ| 32 GB    | 24 GB   | 1 instance   | 10 contexts  | 750       | Researcher prompts
# data_pipeline    | 16 GB  | 80 GB ‚òÖ  | 16 GB   | 5 instances  | 50 contexts ‚òÖ| 1500      | Domain/Page/Facts
# balanced         | 48 GB  | 40 GB    | 32 GB   | 3 instances  | 25 contexts  | 1000      | General dev
# network_friendly | 48 GB  | 40 GB    | 32 GB   | 3 instances  | 5 contexts   | 500       | Shared network
# query_only       | OFF    | 48 GB    | OFF     | 1 instance   | Disabled     | N/A       | Search/exploration
# travel           | N/A    | 32 GB    | N/A     | Mac only     | Disabled     | N/A       | Offline work
#
# ‚òÖ = Priority allocation for this profile
#
# data_pipeline rationale:
#   - Browser pool is SEPARATE from backend - backend doesn't need browser memory
#   - Backend 16GB is 2x base allocation (8GB) - plenty for Python workers
#   - Weaviate 80GB addresses 88% utilization (current usage ~49.5GB)
#   - Browser pool 50 contexts = 2x capacity increase (separate service)
#
# Browser Pool (Spark):
#   - Pool Size: 10-50 contexts (profile dependent)
#   - Memory: 8-48GB limit (data_pipeline: 48GB for 50 contexts)
#   - CPU: 4-14 cores (data_pipeline: 14 cores for 50 contexts)
#   - Available in: ai (reduced), data_pipeline, balanced profiles
#   - SEPARATE SERVICE - not part of backend allocation!
#
# WORKFLOW GUIDE:
#   1. ./scripts/workload.sh data_pipeline  # Process domains, pages, facts
#   2. ./scripts/workload.sh ai             # Run researchers
#   3. ./scripts/workload.sh balanced       # Return to default
#
# -----------------------------------------------------------------------------
