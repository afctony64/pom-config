# =============================================================================
# SHARED CONFIGURATION (Non-Secrets)
# =============================================================================
#
# This file is part of pom-config and is shared across all Pom ecosystem apps.
# Update via: ./scripts/pom-update-all.sh
#
# For secrets (API keys, passwords), use PomSpark/configs/secrets.env.
#
# See: pom-docs/docs/infrastructure/SHARED_CONFIG_GUIDE.md
#
# =============================================================================

# =============================================================================
# SERVICE PORTS (Single Source of Truth)
# =============================================================================
# These ports are the canonical assignments for all Pom ecosystem services.
# Docker Compose files should reference these variables.
# See also: pom-config/services.yaml for full service definitions.

# ----- Frontends -----
POMOTHY_FRONTEND_PORT=5174
POMAI_FRONTEND_PORT=5173
REPORT_SERVER_PORT=8886

# ----- Backends -----
POMOTHY_BACKEND_PORT=8001

# ----- Databases -----
WEAVIATE_PORT=8080
WEAVIATE_GRPC_PORT=50051
REDIS_PORT=6379

# ----- AI Services -----
OLLAMA_LB_PORT=11430
MAC_OLLAMA_NATIVE_PORT=11435
MAC_TRANSFORMERS_NATIVE_PORT=8093

# ----- Supabase -----
SUPABASE_STUDIO_PORT=3100
SUPABASE_KONG_PORT=8100
SUPABASE_DB_PORT=5432

# ----- Observability -----
LOKI_PORT=3102
TEMPO_PORT=3200
JAEGER_UI_PORT=16686

# ----- Dev Tools -----
REDIS_INSIGHT_PORT=5540
MINIO_API_PORT=9000
MINIO_CONSOLE_PORT=9001
MAILHOG_UI_PORT=8025

# ----- Reserved Ports (DO NOT USE) -----
# 4000 - Cursor MCP Server (CRITICAL - will conflict!)
# 5000 - macOS ControlCenter / AirPlay
# 3000 - Common dev server default (React, Next.js)

#+#+#+#+----------------------------------------------------------------------
# RUNTIME DEFAULTS (Applied on fresh startup/recreate)
# ----------------------------------------------------------------------
# These are the CANONICAL defaults. Individual services should NOT override.
# See pom-config/runtime.yaml for full configuration.

# Default workload profile on startup (data_pipeline = heavy embeddings)
DEFAULT_WORKLOAD_PROFILE=data_pipeline

# Page Intelligence per-domain cap (URL discovery)
PI_MAX_URLS_PER_DOMAIN=2000
# Page Facts per-domain cap (scraping limit)
PF_MAX_URLS_PER_DOMAIN=600
# Page Facts completion thresholds
PF_COMPLETION_PERCENT=80
PF_COMPLETION_MIN=500

# Validate configuration after workload profile switch
VALIDATE_ON_SWITCH=true

# Require validation to pass before considering switch complete
REQUIRE_VALIDATION=true

# =============================================================================
# SYSTEM PATHS
# =============================================================================

# Researcher seed data directory (used by pom-core seed loading)
RESEARCHER_SEED_OUTPUT_DIR=/app/PomAI/Data/seeds

# Shared reports directory (centralized reports outside all repos)
# Mac: /Users/tonyeales/Projects/shared-reports
# Spark (container): /shared-reports
SHARED_REPORTS_PATH=/Users/tonyeales/Projects/shared-reports

# Spark shared reports directory (host path)
SPARK_SHARED_REPORTS_PATH=/home/afctony64/Projects/shared-reports

# PomAI reports sync hook (Mac report server on-access sync)
POMAI_REPORTS_SYNC_PORT=8877
POMAI_REPORTS_SYNC_COOLDOWN_SECONDS=120
POMAI_REPORTS_SYNC_TIMEOUT_SECONDS=120

# pom-config shared configuration root
POM_CONFIG_ROOT=/app/shared_config

# =============================================================================
# SYSTEM BEHAVIOR
# =============================================================================
# Note: DEFAULT_WORKLOAD_PROFILE and VALIDATE_ON_SWITCH are in the
# RUNTIME DEFAULTS section above (line ~65)

# Weaviate routing target (which Weaviate instance to use)
# Options: spark (local Spark Weaviate), cloud (Weaviate Cloud), local (Mac Weaviate)
WEAVIATE_MODE=spark

# Default log level for all services
# Use INFO for standard operational visibility, DEBUG for troubleshooting internals
LOG_LEVEL=INFO

# NOTE: File logging removed - all logs go to stdout for docker logs
# Promtail scrapes Docker logs and sends to Loki for dashboards

# =============================================================================
# WEAVIATE CONFIGURATION
# =============================================================================
# Tenant/Collection Routing: Both Mac and Spark workspaces need access to
# tenant instances across both Weaviate instances. Routing is controlled by
# pom-core based on tenant/collection configuration.

# Primary Weaviate URL (current routing target)
# Note: This is overridden per-container via Docker env vars, but pom_core
# loads shared-config.env with override=True (env.py:70) so this value wins.
# Set to the ACTIVE environment's internal Docker hostname.
WEAVIATE_URL=http://spark-weaviate:8080

# Mac Weaviate (local Docker network)
MAC_WEAVIATE_URL=http://mac-weaviate:8080

# Spark Weaviate (Docker-internal hostname)
# NOTE: Use spark-weaviate (Docker DNS) not spark-65d6.local (mDNS) because
# pom_core loads shared-config.env with override=True (env.py:70), overwriting
# Docker Compose env vars. mDNS doesn't resolve inside containers.
# See: pom-core issue #1014
SPARK_WEAVIATE_URL=http://spark-weaviate:8080

# DEPRECATED (was: public DO proxy for Spark services)
# Keep these keys for backward compatibility, but do not set them.
SPARK_WEAVIATE_URL_FALLBACK=
SPARK_WEAVIATE_PROXY_URL=

# Weaviate Cloud
WEAVIATE_CLOUD_URL=https://ewkhvwtiswgp8ugywr8sma.c0.us-east1.gcp.weaviate.cloud
DEFAULT_WEAVIATE_TARGET=cloud

# gRPC (with fallback)
# NOTE: Docker-internal hostname (see SPARK_WEAVIATE_URL comment + pom-core #1014)
WEAVIATE_GRPC_URL=spark-weaviate:50051
SPARK_WEAVIATE_GRPC_URL=spark-weaviate:50051
SPARK_WEAVIATE_GRPC_PROXY_URL=spark-proxy.digitalocean.com:50051

# =============================================================================
# OLLAMA CONFIGURATION (LLM Inference)
# =============================================================================

# Primary Ollama URL (current routing target)
OLLAMA_URL=http://host.docker.internal:11435
OLLAMA_BASE_URL=http://host.docker.internal:11430

# Mac native Ollama
MAC_OLLAMA_URL=http://host.docker.internal:11435

# Spark Ollama (LAN only)
SPARK_OLLAMA_URL=http://spark-65d6.local:11434
# DEPRECATED (was: public DO proxy for Spark services)
SPARK_OLLAMA_PROXY_URL=

# Ollama Load Balancer
OLLAMA_LB_URL=http://host.docker.internal:11430

# Fallback URLs for reliability
OLLAMA_FALLBACK_URLS=http://host.docker.internal:11435,http://spark-65d6.local:11434

# Timeouts (seconds)
OLLAMA_TIMEOUT=900

# =============================================================================
# TRANSFORMERS CONFIGURATION (Embeddings)
# =============================================================================

# Primary Transformers URL (routing target)
#
# Spark-first by default: high-concurrency runs must not hit Mac Metal as the
# primary embeddings backend. Mac remains an explicit fallback.
TRANSFORMERS_URL=http://spark-transformers-lb:80

# Mac Metal transformers
MAC_TRANSFORMERS_URL=http://host.docker.internal:8093

# Spark transformers load balancer (LAN only)
SPARK_TRANSFORMERS_URL=http://spark-transformers-lb:80
# DEPRECATED (was: public DO proxy for Spark services)
SPARK_TRANSFORMERS_PROXY_URL=

# =============================================================================
# TOOLING CONFIGURATION
# =============================================================================

SETUPTOOLS_SCM_PRETEND_VERSION=5.1.88

# =============================================================================
# REDIS CONFIGURATION
# =============================================================================
# NOTE: REDIS_URL is intentionally NOT set here - it's deployment-specific.
# Mac containers use mac-redis, Spark containers use spark-redis.
# Each docker-compose file sets the correct REDIS_URL via environment variables.

# =============================================================================
# SUPABASE CONFIGURATION (URLs only - keys in configs/secrets.env)
# =============================================================================

SUPABASE_URL=https://mhgpujddrrxwlfoyegxg.supabase.co

# =============================================================================
# FRONTEND CONFIGURATION (VITE)
# =============================================================================

VITE_API_BASE_URL=http://localhost:8001
VITE_SERVICE_URL=http://localhost:8001
VITE_WS_URL=ws://localhost:8001
VITE_BACKEND_URL=http://localhost:8001
FRONTEND_URL=http://localhost:5174

# Supabase frontend (anon key in configs/secrets.env)
VITE_SUPABASE_URL=https://mhgpujddrrxwlfoyegxg.supabase.co

# =============================================================================
# GCS BACKUP CONFIGURATION
# =============================================================================

GCS_BACKUP_BUCKET=pomothy-weaviate-backups-gcs
# Legacy alias - prefer GCS_BACKUP_BUCKET for new code
BACKUP_BUCKET=${GCS_BACKUP_BUCKET}

# =============================================================================
# VAST.AI CONFIGURATION
# =============================================================================

VAST_ENDPOINT_NAME=vLLM-Qwen3-8B

# =============================================================================
# OMADA CONTROLLER (URLs only - credentials in configs/secrets.env)
# =============================================================================

OMADA_CONTROLLER_URL=https://192.168.0.229
OMADA_SITE=Default

# =============================================================================
# SECURITY CONFIGURATION
# =============================================================================

# SSRF protection (disable only for trusted internal services)
DISABLE_SSRF_PROTECTION=false

# SSL verification for HTTPX client
HTTPX_VERIFY_SSL=true

# URL validation for web scraping
ENABLE_URL_VALIDATION=true

# =============================================================================
# HTTP CLIENT CONFIGURATION
# =============================================================================

# Default timeout for HTTP requests (seconds)
REQUESTS_TIMEOUT=30

# Maximum content size for downloaded pages (bytes)
MAX_CONTENT_SIZE=300000

# ----- aiohttp Connection Pool (Web Scraping) -----
# Total concurrent HTTP connections (aggressive - Spark can handle 262K conntrack)
HTTP_CONNECTION_LIMIT=30000

# Max connections per domain (politeness limit)
HTTP_LIMIT_PER_HOST=10

# Keep idle connections open for reuse (seconds)
HTTP_KEEPALIVE_TIMEOUT=5

# ----- Web Concurrency (Global Semaphores) -----
# Max domains to scrape in parallel (main throughput control)
# - Aligns with browser pool (500) and HTTP pool capacity
WEB_CONCURRENT_DOMAINS=500

# Max concurrent requests per domain (politeness)
WEB_PER_DOMAIN_MAX_CONCURRENT=5

# ----- Gateway NAT Protection -----
# Max unique destination hosts to protect Cox Gateway (~8K NAT limit)
# Set to 6000 for safety margin (75% of gateway capacity)
MAX_UNIQUE_DESTINATIONS=6000

# Enable/disable the gateway protection guard
DESTINATION_GUARD_ENABLED=true

# ----- External Service Gateway Rate Tuning -----
# Controls pom-core ExternalServiceGateway dispatch rates per service and provider.
# Set to 90% of plan limits to avoid hitting hard caps.
#
# Hierarchical policy resolution (most specific wins):
#   GATEWAY_{SERVICE}_{PROVIDER}_{SETTING}  (provider-specific override)
#   GATEWAY_{SERVICE}_{SETTING}             (service default fallback)
#   Built-in default                        (100 req/s, 100 concurrency)
#
# See: pom-core#921 for design details
#
# ─── LLM ─────────────────────────────────────────────────
# Service default (fallback for any LLM provider without specific config)
GATEWAY_LLM_REQUESTS_PER_SECOND=100
GATEWAY_LLM_MAX_CONCURRENCY=100
#
# Per-provider overrides
GATEWAY_LLM_OPENAI_REQUESTS_PER_SECOND=200
GATEWAY_LLM_OPENAI_MAX_CONCURRENCY=1000
GATEWAY_LLM_ANTHROPIC_REQUESTS_PER_SECOND=50
GATEWAY_LLM_ANTHROPIC_MAX_CONCURRENCY=50
GATEWAY_LLM_GROQ_REQUESTS_PER_SECOND=30
GATEWAY_LLM_GROQ_MAX_CONCURRENCY=30
GATEWAY_LLM_GOOGLE_REQUESTS_PER_SECOND=100
GATEWAY_LLM_GOOGLE_MAX_CONCURRENCY=100
GATEWAY_LLM_OLLAMA_REQUESTS_PER_SECOND=10
GATEWAY_LLM_OLLAMA_MAX_CONCURRENCY=4
#
# ─── MCP TOOLS ────────────────────────────────────────────
# Service default
GATEWAY_MCP_REQUESTS_PER_SECOND=20
GATEWAY_MCP_MAX_CONCURRENCY=50
#
# Per-tool overrides
GATEWAY_MCP_BRAVE_REQUESTS_PER_SECOND=18
GATEWAY_MCP_BRAVE_MAX_CONCURRENCY=18
GATEWAY_MCP_EDGAR_REQUESTS_PER_SECOND=5
GATEWAY_MCP_EDGAR_MAX_CONCURRENCY=5
#
# ─── WEAVIATE ─────────────────────────────────────────────
# Per-instance (pom-core#921: wired once gateway supports Weaviate routing)
GATEWAY_WEAVIATE_ENABLED=false
GATEWAY_WEAVIATE_SPARK_REQUESTS_PER_SECOND=100
GATEWAY_WEAVIATE_SPARK_MAX_CONCURRENCY=50
GATEWAY_WEAVIATE_CLOUD_REQUESTS_PER_SECOND=50
GATEWAY_WEAVIATE_CLOUD_MAX_CONCURRENCY=25
#
# ─── BROWSER ─────────────────────────────────────────────
GATEWAY_BROWSER_REQUESTS_PER_SECOND=20
GATEWAY_BROWSER_MAX_CONCURRENCY=10

# =============================================================================
# REDIS CONFIGURATION (extended)
# =============================================================================

# Redis database number (0-15)
REDIS_DB=0

# =============================================================================
# BROWSER POOL CONFIGURATION
# =============================================================================

# =============================================================================
# FIX #941: Consolidated env var naming
# Canonical names use PLAYWRIGHT_ prefix with _MS/_S unit suffixes.
# Legacy names are kept as comments for reference only.
# =============================================================================

# Browser pool size - max concurrent browser contexts
# - Conservative (8GB): 25
# - data_pipeline (48GB): 500 (each context ~100MB with optimizations)
# - Current: 500 (restored after conntrack fix - 5 min timeout prevents network saturation)
# (Legacy: BROWSER_POOL_SIZE — now consolidated into PLAYWRIGHT_POOL_SIZE below)

# Idle timeout before hibernate (seconds)
# (Legacy: BROWSER_IDLE_TIMEOUT)
PLAYWRIGHT_IDLE_TIMEOUT_S=300

# Page load timeout (milliseconds)
# (Legacy: BROWSER_PAGE_TIMEOUT, PLAYWRIGHT_PAGE_TIMEOUT)
PLAYWRIGHT_PAGE_TIMEOUT_MS=90000

# Decodo (SmartProxy) residential proxy - US endpoint for geo-restricted sites
# Used by scraping clients as a WAF bypass fallback.
#
# SECURITY:
# - Do NOT store proxy credentials in pom-config (shared non-secrets repo).
# - Put the full proxy URL (with credentials) in PomSpark/configs/secrets.env:
#     DECODO_RESIDENTIAL_PROXY_URL="http://user:pass@us.decodo.com:10001"
#     PLAYWRIGHT_RESIDENTIAL_PROXY="http://user:pass@us.decodo.com:10001"  # legacy name
#
# Keep only non-secret host/port hints here (optional).
PLAYWRIGHT_RESIDENTIAL_PROXY=
DECODO_RESIDENTIAL_PROXY_HOST=us.decodo.com
DECODO_RESIDENTIAL_PROXY_PORT=10001

# Browser pool URLs
BROWSER_PROXY_URL=http://browser-proxy:80
MAC_BROWSER_POOL_URL=http://mac-browser-pool:4100
SPARK_BROWSER_POOL_URL=http://spark-browser-pool:4100

# =============================================================================
# PLAYWRIGHT POOL (pom-core) - Spark defaults
# =============================================================================
# Keep in sync with Spark browser pool capacity
PLAYWRIGHT_POOL_SIZE=500
# Recycle contexts after N page loads to cap memory growth
PLAYWRIGHT_CONTEXT_MAX_USES=100
# Zombie detection threshold (seconds) — (Legacy: PLAYWRIGHT_ZOMBIE_TIMEOUT)
PLAYWRIGHT_ZOMBIE_TIMEOUT_S=300
# Zombie reaper interval (seconds) — (Legacy: PLAYWRIGHT_ZOMBIE_REAPER_INTERVAL)
PLAYWRIGHT_ZOMBIE_REAPER_INTERVAL_S=60
# Stream safety: break if no output for this long (seconds)
STREAM_IDLE_TIMEOUT=600
# Heartbeat for liveness + redis metrics (seconds)
STREAM_HEARTBEAT_INTERVAL=60

# =============================================================================
# WEAVIATE TUNING
# =============================================================================

# Enable Weaviate Cloud integration
WEAVIATE_CLOUD_ENABLED=true

# Query defaults
QUERY_DEFAULTS_LIMIT=100
QUERY_MAXIMUM_RESULTS=10000

# Disk usage thresholds (percent)
DISK_USE_WARNING_PERCENTAGE=80
DISK_USE_READONLY_PERCENTAGE=95

# Go runtime tuning (Spark Weaviate - profile-overridable)
# GOMEMLIMIT: Soft memory limit for Go runtime (prevents OOM)
# GOGC: Garbage collection target percentage (lower = more frequent GC)
SPARK_WEAVIATE_GOMEMLIMIT=65GiB
SPARK_WEAVIATE_GOGC=50

# =============================================================================
# PIPELINE OBSERVABILITY (Stale Run Detection)
# =============================================================================
# See: pom-docs/docs/infrastructure/CLI_OBSERVABILITY_ARCHITECTURE.md

# Stale run threshold (seconds) - runs with no heartbeat for this long are stale
# 60 seconds: balances responsiveness with allowing slow operations
# (some CLI operations take >30s per item, causing false-positive stale detection)
STALE_RUN_MAX_AGE=60

# How often the exporter checks for stale runs (seconds)
# REAL-TIME: 15 seconds (was 60) - check frequently for responsive dashboard
STALE_RUN_CLEANUP_INTERVAL=15

# Enable automatic stale run cleanup in the Redis exporter
STALE_RUN_CLEANUP_ENABLED=true

# =============================================================================
# POMFLOW CLI CONFIGURATION
# =============================================================================

# Default max concurrent operations for pomflow CLI
# Overrides model card optimal_concurrency when set
POMFLOW_MAX_CONCURRENT=20

# Default max concurrent operations for standard pom-core CLIs.
#   Relationship with WEAVIATE_MAX_INFLIGHT: at startup, all workers
#   fire page_facts queries simultaneously. Burst = CLI_MAX_CONCURRENT × 5.
#   At 50 × 5 = 250 queries through WEAVIATE_MAX_INFLIGHT=16 → ~16s drain.
#   At 100 × 5 = 500 queries → 54s+ drain and LLM tool timeouts.
CLI_MAX_CONCURRENT=50

# =============================================================================
# AI DIAGNOSTICS THRESHOLDS
# =============================================================================

# Slow AI operation warning threshold (milliseconds)
# Operations exceeding this log a warning for optimization consideration
# 90s is normal for multi-stage entity researcher prompts with tool calls
AI_SLOW_OPERATION_THRESHOLD_MS=90000

# Very slow AI operation error threshold (milliseconds)
# Operations exceeding this log an error requiring investigation
AI_VERY_SLOW_OPERATION_THRESHOLD_MS=120000

# =============================================================================
# BRAVE SEARCH CONFIGURATION
# =============================================================================

# Seconds between requests (rate limiting)
BRAVE_RATE_LIMIT=0.1

# Max parallel requests (per instance)
BRAVE_MAX_CONCURRENT=10

# Global semaphore limit for Brave API calls
BRAVE_SEMAPHORE=10

# Default results per query
BRAVE_RESULTS_COUNT=10

# Retry configuration
BRAVE_MAX_RETRIES=3
BRAVE_BACKOFF_FACTOR=2.0

# =============================================================================
# OPENAI HTTPX POOL LIMITS
# =============================================================================

OPENAI_HTTP_MAX_CONNECTIONS=200
OPENAI_HTTP_MAX_KEEPALIVE=200

# =============================================================================
# OLLAMA TUNING (Spark GPU - 128GB RAM)
# =============================================================================

# Concurrent requests per loaded model
# 32 parallel x 2 models = 64 total concurrent requests
OLLAMA_NUM_PARALLEL=32

# Maximum models in RAM (128GB allows 2 large models)
OLLAMA_MAX_LOADED_MODELS=2

# Keep models loaded after last request
OLLAMA_KEEP_ALIVE=1h

# Enable flash attention for memory efficiency
OLLAMA_FLASH_ATTENTION=1

# Maximum queue size for pending requests
OLLAMA_MAX_QUEUE=1000

# =============================================================================
# SPARK SERVER CONFIGURATION (Infrastructure Defaults)
# =============================================================================
# These are the canonical Spark server connection details.
# Scripts should source these instead of hardcoding.

# Spark server hostname (mDNS/Bonjour)
SPARK_HOST=spark-65d6.local

# =============================================================================
# SERVICE DISCOVERY PROBE ORDER
# =============================================================================
# Startup service discovery probes Spark in this priority order:
#   1. US fixed IP (home network)
#   2. UK WiFi IP (travel network)
#   3. mDNS hostname (fallback)
#   4. Mac services (final fallback)
#
# See: PomSpark/scripts/discover-services.sh

# US home network fixed IP
SPARK_PROBE_US_IP=192.168.0.49

# UK WiFi network IP
SPARK_PROBE_UK_IP=192.168.0.43

# mDNS hostname (Bonjour/Avahi)
SPARK_PROBE_MDNS=spark-65d6.local

# Spark SSH username
SPARK_USER=afctony64

# Spark Tailscale hostname (VPN access)
SPARK_TAILSCALE=spark-65d6

# Spark project paths (on Spark server)
SPARK_POMSPARK_PATH=/home/afctony64/Projects/PomSpark
SPARK_POMAI_PATH=/home/afctony64/Projects/PomAI
SPARK_POMOTHY_PATH=/home/afctony64/Projects/Pomothy

# =============================================================================
# CLOUDFLARE TUNNEL CONFIGURATION (Remote SSH access)
# =============================================================================
# Security model: Cloudflare Tunnel is used for SSH access only.
# Spark service APIs (Weaviate/Ollama/Transformers) are NOT exposed via tunnel.

# Cloudflare SSH hostname (set this if you have a stable hostname)
SPARK_SSH_TUNNEL_HOSTNAME=

# Cloudflare tunnel names
CLOUDFLARE_TUNNEL_SPARK=spark-services
CLOUDFLARE_TUNNEL_SPARK_ID=6d4a4972-ec26-4ca3-9df2-59b543b1e45a

# =============================================================================
# SCRAPING EGRESS PROXIES (Anti-blocking / WAF bypass)
# =============================================================================
# These are NOT for Spark service access. They are for outbound web scraping.
#
# Goals:
# - "home_residential": stable egress IP (home Cox) for sites that block datacenters
# - "decodo_residential": rotating residential pool for WAF/bot mitigation
#
# Secrets (proxy URLs with credentials) MUST live in PomSpark/configs/secrets.env:
#   HOME_RESIDENTIAL_PROXY_URL="http://user:pass@host:port"
#   DECODO_RESIDENTIAL_PROXY_URL="http://user:pass@host:port"
#
# Non-secret selection + optional host hints live here.

# Default scraping egress mode.
# Intended behavior:
# - wireguard_do: default scrape egress is the Spark-side WireGuard tunnel to DO
# - decodo_residential: fallback for sites that block WireGuard/datacenter egress
# - none: direct egress (not recommended for scraping)
#
# Options: wireguard_do, decodo_residential, none
SCRAPE_EGRESS_MODE=wireguard_do

# Expected public egress IP when WireGuard DO tunnel is active.
SCRAPE_WIREGUARD_EGRESS_IP=142.93.80.131

# DO WireGuard gateway details (non-secret)
SCRAPE_WIREGUARD_DO_GATEWAY_IP=142.93.80.131
SCRAPE_WIREGUARD_DO_GATEWAY_WG_IP=10.66.66.1

# Optional non-secret hints (do not include credentials here)
HOME_RESIDENTIAL_PROXY_HOST=
HOME_RESIDENTIAL_PROXY_PORT=
DECODO_RESIDENTIAL_PROXY_HOST=
DECODO_RESIDENTIAL_PROXY_PORT=

# =============================================================================
# WEAVIATE CONCURRENCY (#922, #930)
# =============================================================================
# FIX #930: Weaviate concurrency is managed by a per-instance semaphore in
# database_factory.py, NOT by the external gateway. Weaviate has no server-side
# rate limiter, so we must bound in-flight requests client-side.
#
# WEAVIATE_MAX_INFLIGHT: Max concurrent async operations per Weaviate instance.
#   Tested at 16, 32, 64 with 100 records (Feb 2026):
#     16 → 230s, 17 errors | 32 → 240s, 26 errors | 64 → 256s, 61 errors
#   Higher values saturate Weaviate and increase LLM tool timeouts.
#   Relationship: startup_burst = CLI_MAX_CONCURRENT × 5 queries/record
#   At 50×5=250 queries through 16 slots → ~16s drain (tolerable).
WEAVIATE_MAX_INFLIGHT=16

# Async client pool size (#922) - max distinct Weaviate configs cached
WEAVIATE_ASYNC_POOL_SIZE=20
